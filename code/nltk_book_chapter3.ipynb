{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Processing Raw Text](http://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Accessing Text from the Web and from Disk\n",
    "\n",
    "### 3.1.1 Electronic Books\n",
    "\n",
    "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection. However, you may be interested in analyzing other texts from Project Gutenberg. You can browse the catalog of 25,000 free online books at http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project Gutenberg are in English, it includes material in over 50 other languages, including Catalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese and Spanish (with more than 100 texts each).\n",
    "\n",
    "Text number 2554 is an English translation of Crime and Punishment, and we can access it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    ">>> from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'str'>\n1176967\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(type(raw))\n",
    "print(len(raw))\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable raw contains a string with 1,176,893 characters. (We can see that it is a string, using type(raw).) This is the raw content of the book, including many details we are not interested in such as whitespace, line breaks and blank lines. Notice the \\r and \\n in the opening line of the file, which is how Python displays the special carriage return and line feed characters (the file must have been created on a Windows machine). For our language processing, we want to break up the string into words and punctuation, as we saw in 1.. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "type(tokens)\n",
    "len(tokens)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing we saw in Chapter 1, along with the regular list operations like slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'nltk.text.Text'>\n--------------------------------------------------\n['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n--------------------------------------------------\n['Katerina Ivanovna', 'Pyotr Petrovitch', 'Pulcheria Alexandrovna', 'Avdotya Romanovna', 'Rodion Romanovitch', 'Marfa Petrovna', 'Sofya Semyonovna', 'old woman', 'Project Gutenberg-tm', 'Porfiry Petrovitch', 'Amalia Ivanovna', 'great deal', 'young man', 'Nikodim Fomitch', 'Ilya Petrovitch', 'Project Gutenberg', 'Andrey Semyonovitch', 'Hay Market', 'Dmitri Prokofitch', 'Good heavens']\n"
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(\"-\"*50)\n",
    "print(text[1024:1062])\n",
    "print(\"-\"*50)\n",
    "print(text.collocation_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. Sometimes this information appears in a footer at the end of the file. **We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5336\n1157812\n"
    }
   ],
   "source": [
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(raw.find(\"PART I\"))\n",
    "print(raw.rfind(\"End of Project Gutenberg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n"
    }
   ],
   "source": [
    "raw = raw[raw.find(\"PART I\"):raw.rfind(\"End of Project Gutenberg\")]\n",
    "print(raw.find(\"PART I\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() and rfind() (\"reverse find\") methods help us get the right index values to use for slicing the string [1]. We overwrite raw with this slice, so now it begins with \"PART I\" and goes up to (but not including) the phrase that marks the end of the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get text out of HTML we will use a Python library called BeautifulSoup, available from http://www.crummy.com/software/BeautifulSoup/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK', 'Friday', ',', '27', 'September', ',', '2002', ',', '11:51', 'GMT', '12:51', 'UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', '.', \"''\", 'See', 'also', ':', '28', 'Mar', '01', '|', 'Education', 'What', 'is', 'it', 'about', 'blondes', '?', '09', 'Apr', '99', '|', 'Health', 'Platinum', 'blondes', 'are', 'labelled', 'as', 'dumb', '17', 'Apr', '02', '|', 'Health', 'Hair', 'dye', 'cancer', 'alert', 'Internet', 'links', ':', 'University', 'of', 'Edinburgh', 'The', 'BBC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'external', 'internet', 'sites', 'Top', 'Health', 'stories', 'now', ':', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'Links', 'to', 'more', 'Health', 'stories', 'are', 'at', 'the', 'foot', 'of', 'the', 'page', '.', 'E-mail', 'this', 'story', 'to', 'a', 'friend', 'Links', 'to', 'more', 'Health', 'stories', 'In', 'This', 'Section', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'How', 'sperm', 'wriggle', 'Bollywood', 'told', 'to', 'stub', 'it', 'out', 'Fears', 'over', 'tuna', 'health', 'risk', 'to', 'babies', 'Public', 'can', 'be', 'taught', 'to', 'spot', 'strokes', '^^', 'Back', 'to', 'top', 'News', 'Front', 'Page', '|', 'Africa', '|', 'Americas', '|', 'Asia-Pacific', '|', 'Europe', '|', 'Middle', 'East', '|', 'South', 'Asia', '|', 'UK', '|', 'Business', '|', 'Entertainment', '|', 'Science/Nature', '|', 'Technology', '|', 'Health', '|', 'Talking', 'Point', '|', 'Country', 'Profiles', '|', 'In', 'Depth', '|', 'Programmes', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'To', 'BBC', 'Sport', '>', '>', '|', 'To', 'BBC', 'Weather', '>', '>', '|', 'To', 'BBC', 'World', 'Service', '>', '>', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '©', 'MMIII', '|', 'News', 'Sources', '|', 'Privacy']\n"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still contains unwanted material concerning site navigation and related stories. With some trial and error you can find the start and end indexes of the content and select the tokens of interest, and initialize a text as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcordance\u001b[0m    \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m79\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m\nPrints a concordance for ``word`` with the specified context window.\nWord matching is not case-sensitive.\n\n:param word: The target word\n:type word: str\n:param width: The width of each line, in characters (default=80)\n:type width: int\n:param lines: The number of lines to display (default=25)\n:type lines: int\n\n:seealso: ``ConcordanceIndex``\n\u001b[1;31mFile:\u001b[0m      c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\nltk\\text.py\n\u001b[1;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "?nltk.Text.concordance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Displaying 5 of 5 matches:\nhey say too few people now carry the gene for blondes to last beyond the next \nblonde hair is caused by a recessive gene . In order for a child to have blond\n have blonde hair , it must have the gene on both sides of the family in the g\nere is a disadvantage of having that gene or by chance . They do n't disappear\ndes would disappear is if having the gene was a disadvantage and I do not thin\n"
    }
   ],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Processing Search Engine Results\n",
    "\n",
    "The web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large set of documents, you are more likely to find any linguistic pattern you are interested in. Furthermore, you can make use of very specific patterns, which would only match one or two examples on a smaller example, but which might match tens of thousands of examples when run on the web. A second advantage of web search engines is that they are very easy to use. Thus, they provide a very convenient tool for quickly checking a theory, to see if it is reasonable.\n",
    "\n",
    "Unfortunately, search engines have some significant shortcomings. First, the allowable range of search patterns is severely restricted. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only allow you to search for individual words or strings of words, sometimes with wildcards. Second, search engines give inconsistent results, and can give widely different figures when used at different times or in different geographical regions. When content has been duplicated across multiple sites, search results may be boosted. Finally, the markup in the result returned by a search engine may change unpredictably, breaking any pattern-based method of locating particular content **(a problem which is ameliorated by the use of search engine APIs)**  --> **AKA use API over web scraping or search scraping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Processing RSS Feeds\n",
    "\n",
    "The blogosphere is an important source of text, in both formal and informal registers. With the help of a Python library called the Universal Feed Parser, available from https://pypi.python.org/pypi/feedparser, we can access the content of a blog, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'language': 'en-US',\n 'title': 'Language Log',\n 'title_detail': {'type': 'text/plain',\n  'language': 'en-US',\n  'base': 'https://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n  'value': 'Language Log'},\n 'subtitle': '',\n 'subtitle_detail': {'type': 'text/plain',\n  'language': 'en-US',\n  'base': 'https://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n  'value': ''},\n 'updated': '2020-05-14T10:18:20Z',\n 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=5, tm_mday=14, tm_hour=10, tm_min=18, tm_sec=20, tm_wday=3, tm_yday=135, tm_isdst=0),\n 'links': [{'rel': 'alternate',\n   'type': 'text/html',\n   'href': 'https://languagelog.ldc.upenn.edu/nll'},\n  {'rel': 'self',\n   'type': 'application/atom+xml',\n   'href': 'https://languagelog.ldc.upenn.edu/nll/?feed=atom'}],\n 'link': 'https://languagelog.ldc.upenn.edu/nll',\n 'id': 'https://languagelog.ldc.upenn.edu/nll/?feed=atom',\n 'guidislink': False}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import feedparser\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")  ## Returns Dictionary\n",
    "llog['feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llog.entries[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Goonerisms spalore!\nGoonerisms spalore!\nGoonerisms spalore!\n"
    }
   ],
   "source": [
    "len(llog.entries)\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "print(post['title'])\n",
    "print(post.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<p>\"Prinderella and the Cince Told by Cynthia Hall Domine\"</p>\n<p><iframe title=\"Prinderella and the Cince Told by Cynthia Hall Domine\" width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/NbZd\n"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "print(content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['``', 'Prinderella', 'and', 'the', 'Cince', 'Told', 'by', 'Cynthia', 'Hall', 'Domine', \"''\", 'Is', \"n't\", 'that', 'a', 'shirty', 'dame', '?', 'Selected', 'readings', \"''\", 'Obscene', 'spoonerism', 'and', 'stupid', 'verbing', 'discussion', 'on', 'Radio', '4', \"''\", '(', '12/6/10', ')', \"''\", 'Thematic', 'spoonerisms', '?', \"''\", '(', '7/14/19', ')', \"''\", 'Finger', 'spoonerisms', 'and', 'conservation', 'of', 'caps', \"''\", '(', '6/23/10', ')', \"''\", 'More', 'possible', 'than', 'they', 'can', 'powerfully', 'imagine', \"''\", '(', '7/31/09', ')', \"''\", \"'The\", 'travesty', 'that', 'is', 'taking', 'fold', \"'\", \"''\", '(', '10/28/19', ')', \"''\", 'Nuckin', 'Futs', \"''\", '(', '1/18/12', ')']\n"
    }
   ],
   "source": [
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(word_tokenize(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Reading Local Files\n",
    "\n",
    "In order to read a local file, we need to use Python's built-in open() function, followed by the read() method. Suppose you have a file document.txt, you can load its contents like this:\n",
    "\n",
    "To check that the file that you are trying to open is really in the right directory, use IDLE's Open command in the File menu; this will display a list of all the files in the directory where IDLE is running. An alternative is to examine the current directory from within Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('document.txt')\n",
    "# raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['.ipynb_checkpoints',\n 'nltk_book_chapter1.ipynb',\n 'nltk_book_chapter2.ipynb',\n 'nltk_book_chapter3.ipynb',\n 'test_install.ipynb',\n 'week1_nlp.ipynb']"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible problem you might have encountered when accessing a text file is the newline conventions, which are different for different operating systems. The built-in open() function has a second parameter for controlling how the file is opened: open('document.txt', 'rU') — 'r' means to open the file for reading (the default), and 'U' stands for \"Universal\", which lets us ignore the different conventions used for marking newlines.\n",
    "\n",
    "Assuming that you can open the file, there are several methods for reading it. The read() method creates a string with the contents of the entire file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read a file one line at a time using a for loop. Here we use the strip() method to remove the newline character at the end of the input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('document.txt', 'rU')\n",
    "# for line in f:\n",
    "#     print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2   Strings: Text Processing at the Lowest Level\n",
    "\n",
    "Skipping this section since it is just talking about string manipulation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3   Text Processing with Unicode\n",
    "\n",
    "Our programs will often need to deal with different languages, and different character sets. The concept of \"plain text\" is a fiction. If you live in the English-speaking world you probably use ASCII, possibly without realizing it. If you live in Europe you might use one of the extended Latin character sets, containing such characters as \"ø\" for Danish and Norwegian, \"ő\" for Hungarian, \"ñ\" for Spanish and Breton, and \"ň\" for Czech and Slovak. In this section, we will give an overview of how to use Unicode for processing texts that use non-ASCII character sets.\n",
    "What is Unicode?\n",
    "\n",
    "**Unicode supports over a million characters. Each character is assigned a number, called a code point**. In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal form.\n",
    "\n",
    "Within a program, we can manipulate Unicode strings just like normal strings. **However, when Unicode characters are stored in files or displayed on a terminal, they must be encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a single byte per code point, so they can only support a small subset of Unicode, enough for a single language. Other encodings (such as UTF-8) use multiple bytes and can represent the full range of Unicode characters.**\n",
    "\n",
    "**Text in files will be in a particular encoding, so we need some mechanism for translating it into Unicode — translation into Unicode is called decoding. Conversely, to write out Unicode to a file or a terminal, we first need to translate it into a suitable encoding — this translation out of Unicode is called encoding, and is illustrated in 3.3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Extracting encoded text\n",
    "\n",
    "Let's assume that we have a small text file, and that we know how it is encoded. For example, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish Wikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as Latin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pruska Biblioteka Pañstwowa. Jej dawne zbiory znane pod nazw±\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\nNiemców pod koniec II wojny ¶wiatowej na Dolny ¦l±sk, zosta³y\nodnalezione po 1945 r. na terytorium Polski. Trafi³y do Biblioteki\nJagielloñskiej w Krakowie, obejmuj± ponad 500 tys. zabytkowych\narchiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
    }
   ],
   "source": [
    "# Opening without encoding causes issues with some of the characters.\n",
    "f = open(path)\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python open() function can read encoded data into Unicode strings, and write out Unicode strings in encoded form. It takes a parameter to specify the encoding of the file being read or written. So let's open our Polish file with the encoding 'latin2' and inspect the contents of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\nNiemców pod koniec II wojny światowej na Dolny Śląsk, zostały\nodnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\nJagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\narchiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this does not display correctly on your terminal, or if we want to see the underlying numerical values (or \"codepoints\") of the characters, then we can convert all non-ASCII characters into their two-digit \\xXX and four-digit \\uXXXX representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\nb'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\nb'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\nb'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\nb'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\nb'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4   Regular Expressions for Detecting Word Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded']\n"
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('ed$', w)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Using Basic Meta-Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The . wildcard symbol matches any single character. Suppose we have room in a crossword puzzle for an 8-letter word with j as its third letter and t as its sixth letter. In place of each blank cell we use a period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector']\n"
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^..j..t..$', w)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ? symbol specifies that the previous character is optional. Thus \"^e-?mail\\$\" will match both email and e-mail. We could count the total number of occurrences of this word (in either spelling) in a text using sum(1 for w in text if `re.search('^e-?mail$', w))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Ranges and Closures\n",
    "\n",
    "The T9 system is used for entering text on mobile phones (see 3.5). Two or more words that are entered with the same sequence of keystrokes are known as textonyms. For example, both hole and golf are entered by pressing the sequence 4653. What other words could be produced with the same sequence? Here we use the regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['gold', 'golf', 'hold', 'hole']"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the + symbol a bit further. Notice that it can be applied to individual letters, or to bracketed sets of letters. It should be clear that + simply means \"one or more instances of the preceding item\", which could be an individual character like m, a set like [fed] or a range like [d-f]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "print([w for w in chat_words if re.search('^m+i+n+e+$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha']\n"
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('^[ha]+$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace + with \\*, which means \"zero or more instances of the preceding item\". The regular expression ^m*i*n*e*\\$ will match everything that we found using «^m+i+n+e+$», but also words where some of the letters don't appear at all, e.g. me, min, and mmmmm. Note that the + and * symbols are sometimes referred to as Kleene closures, or simply closures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ^ operator has another function when it appears as the first character inside square brackets. For example \"\\[^aeiouAEIOU]\" matches any character other than a vowel**. We can search the NPS Chat Corpus for words that are made up entirely of non-vowel characters using \"^\\[^aeiouAEIOU]+$\" to find items like these: :):):), grrr, cyb3r and zzzzzzzz. Notice this includes non-alphabetic characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5   Useful Applications of Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe",
   "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}