{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Processing Raw Text](http://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Accessing Text from the Web and from Disk\n",
    "\n",
    "### 3.1.1 Electronic Books\n",
    "\n",
    "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection. However, you may be interested in analyzing other texts from Project Gutenberg. You can browse the catalog of 25,000 free online books at http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project Gutenberg are in English, it includes material in over 50 other languages, including Catalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese and Spanish (with more than 100 texts each).\n",
    "\n",
    "Text number 2554 is an English translation of Crime and Punishment, and we can access it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    ">>> from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'str'>\n1176967\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(type(raw))\n",
    "print(len(raw))\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable raw contains a string with 1,176,893 characters. (We can see that it is a string, using type(raw).) This is the raw content of the book, including many details we are not interested in such as whitespace, line breaks and blank lines. Notice the \\r and \\n in the opening line of the file, which is how Python displays the special carriage return and line feed characters (the file must have been created on a Windows machine). For our language processing, we want to break up the string into words and punctuation, as we saw in 1.. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "type(tokens)\n",
    "len(tokens)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing we saw in Chapter 1, along with the regular list operations like slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'nltk.text.Text'>\n--------------------------------------------------\n['an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.', 'He', 'had', 'successfully']\n--------------------------------------------------\n['Katerina Ivanovna', 'Pyotr Petrovitch', 'Pulcheria Alexandrovna', 'Avdotya Romanovna', 'Rodion Romanovitch', 'Marfa Petrovna', 'Sofya Semyonovna', 'old woman', 'Project Gutenberg-tm', 'Porfiry Petrovitch', 'Amalia Ivanovna', 'great deal', 'young man', 'Nikodim Fomitch', 'Ilya Petrovitch', 'Project Gutenberg', 'Andrey Semyonovitch', 'Hay Market', 'Dmitri Prokofitch', 'Good heavens']\n"
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(\"-\"*50)\n",
    "print(text[1024:1062])\n",
    "print(\"-\"*50)\n",
    "print(text.collocation_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. Sometimes this information appears in a footer at the end of the file. **We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5336\n1157812\n"
    }
   ],
   "source": [
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(raw.find(\"PART I\"))\n",
    "print(raw.rfind(\"End of Project Gutenberg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n"
    }
   ],
   "source": [
    "raw = raw[raw.find(\"PART I\"):raw.rfind(\"End of Project Gutenberg\")]\n",
    "print(raw.find(\"PART I\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The find() and rfind() (\"reverse find\") methods help us get the right index values to use for slicing the string [1]. We overwrite raw with this slice, so now it begins with \"PART I\" and goes up to (but not including) the phrase that marks the end of the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get text out of HTML we will use a Python library called BeautifulSoup, available from http://www.crummy.com/software/BeautifulSoup/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK', 'Friday', ',', '27', 'September', ',', '2002', ',', '11:51', 'GMT', '12:51', 'UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', '.', \"''\", 'See', 'also', ':', '28', 'Mar', '01', '|', 'Education', 'What', 'is', 'it', 'about', 'blondes', '?', '09', 'Apr', '99', '|', 'Health', 'Platinum', 'blondes', 'are', 'labelled', 'as', 'dumb', '17', 'Apr', '02', '|', 'Health', 'Hair', 'dye', 'cancer', 'alert', 'Internet', 'links', ':', 'University', 'of', 'Edinburgh', 'The', 'BBC', 'is', 'not', 'responsible', 'for', 'the', 'content', 'of', 'external', 'internet', 'sites', 'Top', 'Health', 'stories', 'now', ':', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'Links', 'to', 'more', 'Health', 'stories', 'are', 'at', 'the', 'foot', 'of', 'the', 'page', '.', 'E-mail', 'this', 'story', 'to', 'a', 'friend', 'Links', 'to', 'more', 'Health', 'stories', 'In', 'This', 'Section', 'Heart', 'risk', 'link', 'to', 'big', 'families', 'Back', 'pain', 'drug', \"'may\", 'aid', \"diabetics'\", 'Congo', 'Ebola', 'outbreak', 'confirmed', 'Vegetables', 'ward', 'off', \"Alzheimer's\", 'Polio', 'campaign', 'launched', 'in', 'Iraq', 'Gene', 'defect', 'explains', 'high', 'blood', 'pressure', 'Botox', \"'may\", 'cause', 'new', \"wrinkles'\", 'Alien', \"'abductees\", \"'\", 'show', 'real', 'symptoms', 'How', 'sperm', 'wriggle', 'Bollywood', 'told', 'to', 'stub', 'it', 'out', 'Fears', 'over', 'tuna', 'health', 'risk', 'to', 'babies', 'Public', 'can', 'be', 'taught', 'to', 'spot', 'strokes', '^^', 'Back', 'to', 'top', 'News', 'Front', 'Page', '|', 'Africa', '|', 'Americas', '|', 'Asia-Pacific', '|', 'Europe', '|', 'Middle', 'East', '|', 'South', 'Asia', '|', 'UK', '|', 'Business', '|', 'Entertainment', '|', 'Science/Nature', '|', 'Technology', '|', 'Health', '|', 'Talking', 'Point', '|', 'Country', 'Profiles', '|', 'In', 'Depth', '|', 'Programmes', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'To', 'BBC', 'Sport', '>', '>', '|', 'To', 'BBC', 'Weather', '>', '>', '|', 'To', 'BBC', 'World', 'Service', '>', '>', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '©', 'MMIII', '|', 'News', 'Sources', '|', 'Privacy']\n"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still contains unwanted material concerning site navigation and related stories. With some trial and error you can find the start and end indexes of the content and select the tokens of interest, and initialize a text as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcordance\u001b[0m    \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m79\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m\nPrints a concordance for ``word`` with the specified context window.\nWord matching is not case-sensitive.\n\n:param word: The target word\n:type word: str\n:param width: The width of each line, in characters (default=80)\n:type width: int\n:param lines: The number of lines to display (default=25)\n:type lines: int\n\n:seealso: ``ConcordanceIndex``\n\u001b[1;31mFile:\u001b[0m      c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\nltk\\text.py\n\u001b[1;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "?nltk.Text.concordance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Displaying 5 of 5 matches:\nhey say too few people now carry the gene for blondes to last beyond the next \nblonde hair is caused by a recessive gene . In order for a child to have blond\n have blonde hair , it must have the gene on both sides of the family in the g\nere is a disadvantage of having that gene or by chance . They do n't disappear\ndes would disappear is if having the gene was a disadvantage and I do not thin\n"
    }
   ],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Processing Search Engine Results\n",
    "\n",
    "The web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large set of documents, you are more likely to find any linguistic pattern you are interested in. Furthermore, you can make use of very specific patterns, which would only match one or two examples on a smaller example, but which might match tens of thousands of examples when run on the web. A second advantage of web search engines is that they are very easy to use. Thus, they provide a very convenient tool for quickly checking a theory, to see if it is reasonable.\n",
    "\n",
    "Unfortunately, search engines have some significant shortcomings. First, the allowable range of search patterns is severely restricted. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only allow you to search for individual words or strings of words, sometimes with wildcards. Second, search engines give inconsistent results, and can give widely different figures when used at different times or in different geographical regions. When content has been duplicated across multiple sites, search results may be boosted. Finally, the markup in the result returned by a search engine may change unpredictably, breaking any pattern-based method of locating particular content **(a problem which is ameliorated by the use of search engine APIs)**  --> **AKA use API over web scraping or search scraping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Processing RSS Feeds\n",
    "\n",
    "The blogosphere is an important source of text, in both formal and informal registers. With the help of a Python library called the Universal Feed Parser, available from https://pypi.python.org/pypi/feedparser, we can access the content of a blog, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'language': 'en-US',\n 'title': 'Language Log',\n 'title_detail': {'type': 'text/plain',\n  'language': 'en-US',\n  'base': 'https://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n  'value': 'Language Log'},\n 'subtitle': '',\n 'subtitle_detail': {'type': 'text/plain',\n  'language': 'en-US',\n  'base': 'https://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n  'value': ''},\n 'updated': '2020-05-15T01:00:58Z',\n 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=5, tm_mday=15, tm_hour=1, tm_min=0, tm_sec=58, tm_wday=4, tm_yday=136, tm_isdst=0),\n 'links': [{'rel': 'alternate',\n   'type': 'text/html',\n   'href': 'https://languagelog.ldc.upenn.edu/nll'},\n  {'rel': 'self',\n   'type': 'application/atom+xml',\n   'href': 'https://languagelog.ldc.upenn.edu/nll/?feed=atom'}],\n 'link': 'https://languagelog.ldc.upenn.edu/nll',\n 'id': 'https://languagelog.ldc.upenn.edu/nll/?feed=atom',\n 'guidislink': False}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import feedparser\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")  ## Returns Dictionary\n",
    "llog['feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llog.entries[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "How many sides does an equation have?\nHow many sides does an equation have?\nHow many sides does an equation have?\n"
    }
   ],
   "source": [
    "len(llog.entries)\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "print(post['title'])\n",
    "print(post.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<p>Tim Finin writes:</p>\n<p style=\"padding-left: 40px;\">President Trump said Fauci wants to \"play all sides of the equation\" about reopening schools. I thought that was an unusual phrase and used goog\n"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "print(content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Tim', 'Finin', 'writes', ':', 'President', 'Trump', 'said', 'Fauci', 'wants', 'to', '``', 'play', 'all', 'sides', 'of', 'the', 'equation', \"''\", 'about', 'reopening', 'schools', '.', 'I', 'thought', 'that', 'was', 'an', 'unusual', 'phrase', 'and', 'used', 'google', 'to', 'search', 'for', 'it', 'without', 'the', 'token', 'Trump', '.', 'There', 'were', 'three', 'hits', '.', 'Here', 'they', 'are', ':', '[', 'link', ']', 'Whether', 'you', 'switch', 'between', 'or', 'within', 'different', 'relationships', 'or', 'are', 'just', 'curious', 'to', 'see', 'how', 'people', 'play', 'all', 'sides', 'of', 'the', 'equation', ',', 'this', 'class', 'will', 'explore', 'the', 'different', 'aspects', 'and', 'appeal', 'of', 'service', 'topping', '.', '[', 'link', ']', 'I', \"'m\", 'not', 'saying', 'I', \"'m\", 'completely', 'buying', 'into', 'that', 'one', 'plus', 'being', 'worth', 'the', 'cap', 'as', 'well', 'as', 'the', 'fact', 'that', 'my', 'assets', 'are', 'essentially', 'non-liquid', 'for', 'a', 'period', 'of', 'time', ',', 'I', \"'m\", 'just', 'trying', 'to', 'play', 'all', 'sides', 'of', 'the', 'equation', 'here', '.', '[', 'link', ']', 'And', 'remember', ',', 'if', 'you', 'want', 'to', 'be', 'taken', 'seriously', ',', 'go', 'play', 'all', 'sides', 'of', 'the', 'equation', 'that', 'you', 'complain', 'about', '.', 'For', 'Donald', 'Trump', \"'s\", 'version', ',', 'see', 'e.g', '.', 'Caitlin', 'Oprysko', ',', '``', 'Trump', 'tweaks', 'Fauci', 'on', 'school', 'reopenings', ':', \"'He\", 'wants', 'to', 'play', 'all', 'sides', 'of', 'the', 'equation', \"'\", \"''\", ',', 'Politico', '5/13/2020', '.', 'The', 'relevant', 'passage', '—', 'in', 'which', 'the', 'reporter', 'repeats', 'the', 'expression', 'using', '``', 'both', 'sides', \"''\", 'rather', 'than', '``', 'all', 'sides', \"''\", ':', 'Your', 'browser', 'does', 'not', 'support', 'the', 'audio', 'element', '.', 'Q', ':', 'Dr.', 'Fauci', 'yesterday', 'was', 'a', 'little', 'cautious', 'on', 're-opening', 'the', 'economy', 'too', 'soon', 'uh', 'do', 'you', 'share', 'his', 'concerns', '?', 'A', ':', 'About', 're-opening', 'what', '?', 'Q', ':', 'Re-opening', 'the', 'economy', 'too', 'soon', 'in', 'some', 'states', 'A', ':', 'Look', 'he', 'wants', 'to', 'play', 'all', 'sides', 'of', 'the', 'equation', 'uh', 'I', 'think', 'we', \"'re\", 'going', 'to', 'have', 'tremendous', 'fourth', 'quarter', 'I', 'think', 'we', \"'re\", 'going', 'to', 'have', 'a', 'transitional', 'third', 'quarter', 'and', 'I', 'think', 'we', \"'re\", 'going', 'to', 'have', 'a', 'phenomenal', 'next', 'year', '.', 'We', 'want', 'it', 'open', '.', 'Q', ':', 'When', 'you', 'say', 'Dr.', 'Fauci', 'is', 'playing', 'both', 'sides', ',', 'are', 'you', 'suggesting', 'that', 'the', 'advice', 'he', \"'s\", 'giving', 'is', '(', '(', ')', ')', 'A', ':', 'Well', 'I', 'was', 'surprised-', 'I', 'was', 'surprised', 'by', 'his', 'answer', 'actually', 'uh', 'because', 'uh', 'you', 'know', 'uh', 'it', \"'s\", 'just', 'to', 'me', 'it', \"'s\", 'not', 'an', 'acceptable', 'answer', 'especially', 'when', 'it', 'comes', 'to', 'schools', 'Let', \"'s\", 'note', 'that', 'most', 'linguists', 'would', 'take', 'the', 'view', 'that', '``', 'all', 'sides', \"''\", 'can', 'truthfully', 'refer', 'to', 'two', 'sides', ',', 'if', 'that', \"'s\", 'all', 'the', 'sides', 'that', 'there', 'are', '—', 'the', 'oddity', 'of', 'the', 'expression', 'comes', 'from', 'a', 'Gricean', 'implicature', 'generated', 'by', 'the', 'maxim', 'of', 'manner', '(', 'and', 'maybe', 'a', 'bit', 'of', 'the', 'maxim', 'of', 'quantity', ')', ',', 'suggesting', 'that', '``', 'all', \"''\", 'would', 'not', 'be', 'chosen', 'instead', 'of', '``', 'both', \"''\", 'if', 'there', 'were', 'only', 'two', 'sides', '.', 'And', 'both', 'sides', 'of', 'an', 'equation', 'have', 'infinitely', 'many', 'forms', ',', 'in', 'pretty', 'much', 'any', 'algebraic', 'system', ',', 'so', 'there', \"'s\", 'a', 'concept', 'of', '``', 'equation', \"''\", 'that', 'could', 'be', 'said', 'to', 'have', 'infinitely', 'many', 'sides', '.', 'On', 'the', 'other', 'hand', ',', 'there', \"'s\", 'the', 'question', 'of', 'what', 'it', 'means', 'to', '``', 'play', \"''\", 'any', 'side', 'of', 'an', 'equation', '.']\n"
    }
   ],
   "source": [
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(word_tokenize(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Reading Local Files\n",
    "\n",
    "In order to read a local file, we need to use Python's built-in open() function, followed by the read() method. Suppose you have a file document.txt, you can load its contents like this:\n",
    "\n",
    "To check that the file that you are trying to open is really in the right directory, use IDLE's Open command in the File menu; this will display a list of all the files in the directory where IDLE is running. An alternative is to examine the current directory from within Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('document.txt')\n",
    "# raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['.ipynb_checkpoints',\n 'nltk_book_chapter1.ipynb',\n 'nltk_book_chapter2.ipynb',\n 'nltk_book_chapter3.ipynb',\n 'test_install.ipynb',\n 'week1_nlp.ipynb']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible problem you might have encountered when accessing a text file is the newline conventions, which are different for different operating systems. The built-in open() function has a second parameter for controlling how the file is opened: open('document.txt', 'rU') — 'r' means to open the file for reading (the default), and 'U' stands for \"Universal\", which lets us ignore the different conventions used for marking newlines.\n",
    "\n",
    "Assuming that you can open the file, there are several methods for reading it. The read() method creates a string with the contents of the entire file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read a file one line at a time using a for loop. Here we use the strip() method to remove the newline character at the end of the input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('document.txt', 'rU')\n",
    "# for line in f:\n",
    "#     print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2   Strings: Text Processing at the Lowest Level\n",
    "\n",
    "Skipping this section since it is just talking about string manipulation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3   Text Processing with Unicode\n",
    "\n",
    "Our programs will often need to deal with different languages, and different character sets. The concept of \"plain text\" is a fiction. If you live in the English-speaking world you probably use ASCII, possibly without realizing it. If you live in Europe you might use one of the extended Latin character sets, containing such characters as \"ø\" for Danish and Norwegian, \"ő\" for Hungarian, \"ñ\" for Spanish and Breton, and \"ň\" for Czech and Slovak. In this section, we will give an overview of how to use Unicode for processing texts that use non-ASCII character sets.\n",
    "What is Unicode?\n",
    "\n",
    "**Unicode supports over a million characters. Each character is assigned a number, called a code point**. In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal form.\n",
    "\n",
    "Within a program, we can manipulate Unicode strings just like normal strings. **However, when Unicode characters are stored in files or displayed on a terminal, they must be encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a single byte per code point, so they can only support a small subset of Unicode, enough for a single language. Other encodings (such as UTF-8) use multiple bytes and can represent the full range of Unicode characters.**\n",
    "\n",
    "**Text in files will be in a particular encoding, so we need some mechanism for translating it into Unicode — translation into Unicode is called decoding. Conversely, to write out Unicode to a file or a terminal, we first need to translate it into a suitable encoding — this translation out of Unicode is called encoding, and is illustrated in 3.3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Extracting encoded text\n",
    "\n",
    "Let's assume that we have a small text file, and that we know how it is encoded. For example, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish Wikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as Latin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pruska Biblioteka Pañstwowa. Jej dawne zbiory znane pod nazw±\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\nNiemców pod koniec II wojny ¶wiatowej na Dolny ¦l±sk, zosta³y\nodnalezione po 1945 r. na terytorium Polski. Trafi³y do Biblioteki\nJagielloñskiej w Krakowie, obejmuj± ponad 500 tys. zabytkowych\narchiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
    }
   ],
   "source": [
    "# Opening without encoding causes issues with some of the characters.\n",
    "f = open(path)\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python open() function can read encoded data into Unicode strings, and write out Unicode strings in encoded form. It takes a parameter to specify the encoding of the file being read or written. So let's open our Polish file with the encoding 'latin2' and inspect the contents of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\nNiemców pod koniec II wojny światowej na Dolny Śląsk, zostały\nodnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\nJagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\narchiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this does not display correctly on your terminal, or if we want to see the underlying numerical values (or \"codepoints\") of the characters, then we can convert all non-ASCII characters into their two-digit \\xXX and four-digit \\uXXXX representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\nb'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\nb'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\nb'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\nb'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\nb'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
    }
   ],
   "source": [
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4   Regular Expressions for Detecting Word Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded']\n"
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('ed$', w)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Using Basic Meta-Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The . wildcard symbol matches any single character. Suppose we have room in a crossword puzzle for an 8-letter word with j as its third letter and t as its sixth letter. In place of each blank cell we use a period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector']\n"
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search('^..j..t..$', w)][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ? symbol specifies that the previous character is optional. Thus \"^e-?mail\\$\" will match both email and e-mail. We could count the total number of occurrences of this word (in either spelling) in a text using sum(1 for w in text if `re.search('^e-?mail$', w))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Ranges and Closures\n",
    "\n",
    "The T9 system is used for entering text on mobile phones (see 3.5). Two or more words that are entered with the same sequence of keystrokes are known as textonyms. For example, both hole and golf are entered by pressing the sequence 4653. What other words could be produced with the same sequence? Here we use the regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['gold', 'golf', 'hold', 'hole']"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the + symbol a bit further. Notice that it can be applied to individual letters, or to bracketed sets of letters. It should be clear that + simply means \"one or more instances of the preceding item\", which could be an individual character like m, a set like [fed] or a range like [d-f]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "print([w for w in chat_words if re.search('^m+i+n+e+$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', 'hahahahaaa', 'hahahahahaha', 'hahahahahahaha', 'hahahahahahahahahahahahahahahaha', 'hahahhahah', 'hahhahahaha']\n"
    }
   ],
   "source": [
    "print([w for w in chat_words if re.search('^[ha]+$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace + with \\*, which means \"zero or more instances of the preceding item\". The regular expression ^m*i*n*e*\\$ will match everything that we found using «^m+i+n+e+$», but also words where some of the letters don't appear at all, e.g. me, min, and mmmmm. Note that the + and * symbols are sometimes referred to as Kleene closures, or simply closures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ^ operator has another function when it appears as the first character inside square brackets. For example \"\\[^aeiouAEIOU]\" matches any character other than a vowel**. We can search the NPS Chat Corpus for words that are made up entirely of non-vowel characters using \"^\\[^aeiouAEIOU]+$\" to find items like these: :):):), grrr, cyb3r and zzzzzzzz. Notice this includes non-alphabetic characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5   Useful Applications of Regular Expressions\n",
    "\n",
    "### 3.5.1 Extracting Word Pieces\n",
    "\n",
    "The re.findall() (\"find all\") method finds all (non-overlapping) matches of the given regular expression. Let's find all the vowels in a word, then count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n16\n"
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "all_vov = re.findall(\"[aeiouAEIOU]\", word)\n",
    "print(all_vov)\n",
    "print(len(all_vov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for all sequences of two or more vowels in some text, and determine their relative frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('io', 549),\n ('ea', 476),\n ('ie', 331),\n ('ou', 329),\n ('ai', 261),\n ('ia', 253),\n ('ee', 217),\n ('oo', 174),\n ('ua', 109),\n ('au', 106),\n ('ue', 105),\n ('ui', 95)]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                   for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Turn: In the W3C Date Time Format, dates are represented like this: 2009-12-31. Replace the ? in the following Python code with a regular expression, in order to convert the string '2009-12-31' to a list of integers \\[2009, 12, 31]:\n",
    "\n",
    "\\[int(n) for n in re.findall(?, '2009-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2009, 12, 31]"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "[int(n) for n in re.findall(\"[0-9]+\", '2009-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Doing More with Word Pieces\n",
    "\n",
    "Once we can use re.findall() to extract material from words, there's interesting things to do with the pieces, like glue them back together or plot them.\n",
    "\n",
    "It is sometimes noted that English text is highly redundant, and it is still easy to read when word-internal vowels are left out. For example, declaration becomes dclrtn, and inalienable becomes inlnble, retaining any initial or final vowel sequences. The regular expression in our next example matches initial vowel sequences, final vowel sequences, and all consonants; everything else is ignored. **This three-way disjunction is processed left-to-right, if one of the three parts matches the word, any later parts of the regular expression are ignored**. We use re.findall() to extract all the matching pieces, and ''.join() to join them together (see 3.9 for more about the join operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp1 = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "regexp2 = r'^[AEIOUaeiou]+|[^AEIOUaeiou]|[AEIOUaeiou]+$'\n",
    "def compress(word, regexp):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\nof the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\nof frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\nrghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\nand the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n--------------------------------------------------\nUnvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\nof the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\nof frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\nrghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\nand the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
    }
   ],
   "source": [
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w, regexp1) for w in english_udhr[:75]))\n",
    "print(\"-\"*50)\n",
    "print(nltk.tokenwrap(compress(w, regexp2) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's combine regular expressions with conditional frequency distributions. Here we will extract all consonant-vowel sequences from the words of Rotokas, such as ka and si. Since each of these is a pair, it can be used to initialize a conditional frequency distribution. We then tabulate the frequency of each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a   e   i   o   u \nk 418 148  94 420 173 \np  83  31 105  34  51 \nr 187  63  84  89  79 \ns   0   0 100   2   1 \nt  47   8   0 148  37 \nv  93  27 105  48  49 \n"
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to inspect the words behind the numbers in the above table, it would be helpful to have an index, allowing us to quickly find the list of words that contains a given consonant-vowel pair, e.g. cv_index\\['su'] should give us all words containing su. Here's how we can do this:\n",
    "\n",
    "This program processes each word w in turn, and for each one, finds every substring that matches the regular expression \\[ptksvr]\\[aeiou]. In the case of the word kasuari, it finds ka, su and ri. Therefore, the cv_word_pairs list will contain ('ka', 'kasuari'), ('su', 'kasuari') and ('ri', 'kasuari'). **One further step, using nltk.Index(), converts this into a useful index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m     \ndefaultdict(default_factory[, ...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.\n\u001b[1;31mFile:\u001b[0m           c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\nltk\\util.py\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "?nltk.Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['kasuari']\n['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
    }
   ],
   "source": [
    "cv_word_pairs = [(cv, w) \n",
    "                  for w in rotokas_words\n",
    "                  for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)  # creates a dictionary\n",
    "print(cv_index['su'])\n",
    "print(cv_index['po'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Finding Word Stems\n",
    "\n",
    "When we use a web search engine, we usually don't mind (or even notice) if the words in the document differ from our search terms in having different endings. A query for laptops finds documents containing laptop and vice versa. Indeed, laptop and laptops are just two forms of the same dictionary word (or lemma). For some language processing tasks we want to ignore word endings, and just deal with word stems.\n",
    "\n",
    "There are various ways we can pull out the stem of a word. Here's a simple-minded approach which just strips off anything that looks like a suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we will ultimately use NLTK's built-in stemmers, it's interesting to see how we can use regular expressions for this task. Our first step is to build up a disjunction of all the suffixes. **We need to enclose it in parentheses in order to limit the scope of the disjunction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ing']"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we'd actually like to split the word into stem and suffix. So we should just parenthesize both parts of the regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('process', 'ing')]"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks promising, but still has a problem. Let's look at a different word, processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('processe', 's')]"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The regular expression incorrectly found an -s suffix instead of an -es suffix. This demonstrates another subtlety: the star operator is \"greedy\" and the .* part of the expression tries to consume as much of the input as possible. If we use the \"non-greedy\" version of the star operator, written *?, we get what we want:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('process', 'es')]"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works even when we allow an empty suffix, by making the content of the second parentheses optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('language', '')]"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach still has many problems (can you spot them?) but we will move on to define a function to perform stemming, and apply it to a whole text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
    }
   ],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "print([stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Searching Tokenized Text\n",
    "\n",
    "You can use a special kind of regular expression for searching across multiple words in a text (where a text is a list of tokens). For example, \"<a\\> <man\\>\" finds all instances of _a man_ in the text. **The angle brackets are used to mark token boundaries**, and any whitespace between the angle brackets is ignored (behaviors that are unique to NLTK's findall() method for texts). In the following example, we include <.*> which will match any single token, and enclose it in parentheses so only the matched word (e.g. monied) and not the matched phrase (e.g. a monied man) is produced. The second example finds three-word phrases ending with the word bro. The last example finds sequences of three or more words starting with the letter l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "monied; nervous; dangerous; white; white; white; pious; queer; good;\nmature; white; Cape; great; wise; wise; butterless; white; fiendish;\npale; furious; better; certain; complete; dismasted; younger; brave;\nbrave; brave; brave\n"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you rule bro; telling you bro; u twizted bro\n"
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\nla la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to build search patterns when the linguistic phenomenon we're studying is tied to particular words. In some cases, a little creativity will go a long way. For instance, searching a large text corpus for expressions of the form x and other ys allows us to discover hypernyms (cf 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "speed and other activities; water and other liquids; tomb and other\nlandmarks; Statues and other monuments; pearls and other jewels;\ncharts and other items; roads and other features; figures and other\nobjects; military and other areas; demands and other factors;\nabstracts and other compilations; iron and other metals\n"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6   Normalizing Text\n",
    "\n",
    "In earlier program examples we have often converted text to lowercase before doing anything with its words, e.g. set(w.lower() for w in text). By using lower(), we have normalized the text to lowercase so that the distinction between _The_ and _the_ is ignored. Often we want to go further than this, and strip off any affixes, a task known as stemming. **A further step is to make sure that the resulting form is a known word in a dictionary, a task known as lemmatization**. We discuss each of these in turn. First, we need to define the data we will use in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Stemmers\n",
    "\n",
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), while the Lancaster stemmer does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n--------------------------------------------------\n['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(\"-\"*50)\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \ndoctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \nere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\nh it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\nnot stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Lemmatization\n",
    "\n",
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7   Regular Expressions for Tokenizing Text\n",
    "\n",
    "Tokenization is the task of cutting a string into identifiable linguistic units that constitute a piece of language data. Although it is a fundamental task, we have been able to delay it until now because many corpora are already tokenized, and because NLTK includes some tokenizers. Now that you are familiar with regular expressions, you can learn how to use them to tokenize text, and to have much more control over the process.\n",
    "\n",
    "### 3.7.1 Simple Approaches to Tokenization\n",
    "\n",
    "The very simplest method for tokenizing text is to split on whitespace. Consider the following text from Alice's Adventures in Wonderland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could split this raw text on whitespace using raw.split(). To do the same using a regular expression, it is not enough to match any space characters in the string [1] since this results in tokens that contain a \\n newline character; instead we need to match any number of spaces, tabs, or newlines.\n",
    "\n",
    "The regular expression \\[ \\t\\n]+ matches one or more space, tab (\\t) or newline (\\n). Other whitespace characters, such as carriage-return and form-feed should really be included too. Instead, we will use a built-in re abbreviation, \\s, which means any whitespace character. **The above statement can be rewritten as re.split(r'\\s+', raw).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n--------------------------------------------------\n[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n--------------------------------------------------\n[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n"
    }
   ],
   "source": [
    "print(re.split(r' ', raw))\n",
    "print(\"-\"*50)\n",
    "print(re.split(r'[ \\t\\n]+', raw) )\n",
    "print(\"-\"*50)\n",
    "print(re.split(r'\\s+', raw) )  ## Condensed form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "#### Important: Remember to prefix regular expressions with the letter r (meaning \"raw\"), which instructs the Python interpreter to treat the string literally, rather than processing any backslashed characters it contains.\n",
    "\n",
    "Splitting on whitespace gives us tokens like '(not' and 'herself,'. **An alternative is to use the fact that Python provides us with a character class \\w for word characters, equivalent to [a-zA-Z0-9_]. It also defines the complement of this class \\W, i.e. all characters other than letters, digits or underscore. We can use \\W in a simple regular expression to split the input on anything other than a word character:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n"
    }
   ],
   "source": [
    "print(re.split(r'\\W+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this gives us empty strings at the start and the end (to understand why, try doing 'xx'.split('x')). We get the same tokens, but without the empty strings, with re.findall(r'\\w+', raw), using a pattern that matches the words instead of the spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['', '', '']\n['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
    }
   ],
   "source": [
    "print('xx'.split('x'))\n",
    "print(re.findall(r'\\w+', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're matching the words, we're in a position to extend the regular expression to cover a wider range of cases. The regular expression «\\w+|\\S\\w*» will first try to match any sequence of word characters. If no match is found, it will try to match any non-whitespace character (\\S is the complement of \\s) followed by further word characters. This means that punctuation is grouped with any following letters (e.g. 's) but that sequences of two or more punctuation characters are separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n"
    }
   ],
   "source": [
    "print(re.findall(r'\\w+|\\S\\w*', raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generalize the \\w+ in the above expression to permit word-internal hyphens and apostrophes: `\\w+([-']\\w+)*`. This expression means \\w+ followed by zero or more instances of \\[-']\\w+; it would match hot-tempered and it's. **(We need to include ?: in this expression for reasons discussed earlier. -- <font color='red'> Not sure what this is doing.</font>.) We'll also add a pattern to match quote characters so these are kept separate from the text they enclose.** \n",
    "\n",
    "The expression also included `\\[-.(]+` which causes the double hyphen, ellipsis, and open parenthesis to be tokenized separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n--------------------------------------------------\n['', '', \"'M\", '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"'t\", '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"'s\", '', '', '', '', '', '-tempered', '', '', '']\n"
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))\n",
    "print(\"-\"*50)\n",
    "print(re.findall(r\"\\w+([-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)) # without ?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 NLTK's Regular Expression Tokenizer\n",
    "\n",
    "The function nltk.regexp_tokenize() is similar to re.findall() (as we've been using it for tokenization). However, nltk.regexp_tokenize() is more efficient for this task, and avoids the need for special treatment of parentheses. For readability we break up the regular expression over several lines and add a comment about each line. The special (?x) \"verbose flag\" tells Python to strip out the embedded whitespace and comments.\n",
    "\n",
    "When using the verbose flag, you can no longer use ' ' to match a space character; use \\s instead. The regexp_tokenize() function has an optional gaps parameter. When set to True, the regular expression specifies the gaps between tokens, as with re.split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)       # set flag to allow verbose regexp\n",
    "    (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "    | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.             # ellipsis\n",
    "    | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "#### We can evaluate a tokenizer by comparing the resulting tokens with a wordlist, and reporting any tokens that don't appear in the wordlist, using set(tokens).difference(wordlist). You'll probably want to lowercase all the tokens first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Further Issues with Tokenization\n",
    "\n",
    "Tokenization turns out to be a far more difficult task than you might have expected. No single solution works well across-the-board, and we must decide what counts as a token depending on the application domain.\n",
    "\n",
    "**When developing a tokenizer it helps to have access to raw text which has been manually tokenized, in order to compare the output of your tokenizer with high-quality (or \"gold-standard\") tokens. The NLTK corpus collection includes a sample of Penn Treebank data, including the raw Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).**\n",
    "\n",
    "A final issue for tokenization is the presence of contractions, such as didn't. If we are analyzing the meaning of a sentence, it would probably be more useful to normalize this form to two separate forms: did and n't (or not). We can do this work with the help of a lookup table.\n",
    "\n",
    "## 3.8   Segmentation\n",
    "\n",
    "This section discusses more advanced concepts, which you may prefer to skip on the first time through this chapter.\n",
    "\n",
    "Tokenization is an instance of a more general problem of segmentation. In this section we will look at two other instances of this problem, which use radically different techniques to the ones we have seen so far in this chapter.\n",
    "\n",
    "### 3.8.1 Sentence Segmentation\n",
    "\n",
    "Manipulating texts at the level of individual words often presupposes the ability to divide a text into individual sentences. As we have seen, some corpora already provide access at the sentence level. In the following example, we compute the average number of words per sentence in the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20.250994070456922"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, the text is only available as a stream of characters. Before tokenizing the text into words, we need to segment it into sentences. NLTK facilitates this by including the Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example of its use in segmenting the text of a novel. (Note that if the segmenter's internal data has been updated by the time you read this, you will see different output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[The Man Who Was Thursday by G. K. Chesterton 1908]\n\nTo Edmund Clerihew Bentley\n\nA cloud was on the mind of men, and wailing went the weather,\nYea, a sick cloud upon the soul when we were boys together.\nScience announced nonentity and art admired decay;\nThe world was old and ended: but you and I were gay;\nRound us in antic order their crippled vices came--\nLust that had lost its laughter, fear that had lost its shame.\nLike the white lock of Whistler, that lit our aimless gloom,\nMen showed their own white feather as proudly as a plume.\nLife was a fly that faded, and death a drone that stung;\nThe world was very old indeed when you and I were young.\nThey twisted even decent sin to shapes not to be named:\nMen were ashamed of honour; but we were not ashamed.\nWeak if we were and foolish, not thus we failed, not thus;\nWhen that black Baal blocked the heavens he had no hymns from us\nChildren we were--our forts of sand were even as weak as eve,\nHigh as they went we piled them up to break that b\n"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[The Man Who Was Thursday by G. K. Chesterton 1908]\\n'\n '\\n'\n 'To Edmund Clerihew Bentley\\n'\n '\\n'\n 'A cloud was on the mind of men, and wailing went the weather,\\n'\n 'Yea, a sick cloud upon the soul when we were boys together.',\n 'Science announced nonentity and art admired decay;\\n'\n 'The world was old and ended: but you and I were gay;\\n'\n 'Round us in antic order their crippled vices came--\\n'\n 'Lust that had lost its laughter, fear that had lost its shame.',\n 'Like the white lock of Whistler, that lit our aimless gloom,\\n'\n 'Men showed their own white feather as proudly as a plume.',\n 'Life was a fly that faded, and death a drone that stung;\\n'\n 'The world was very old indeed when you and I were young.',\n 'They twisted even decent sin to shapes not to be named:\\n'\n 'Men were ashamed of honour; but we were not ashamed.',\n 'Weak if we were and foolish, not thus we failed, not thus;\\n'\n 'When that black Baal blocked the heavens he had no hymns from us\\n'\n 'Children we were--our forts of sand were even as weak as eve,\\n'\n 'High as they went we piled them up to break that bitter sea.',\n 'Fools as we were in motley, all jangling and absurd,\\n'\n 'When all church bells were silent our cap and beds were heard.',\n 'Not all unhelped we held the fort, our tiny flags unfurled;\\n'\n 'Some giants laboured in that cloud to lift it from the world.',\n 'I find again the book we found, I feel the hour that flings\\n'\n 'Far out of fish-shaped Paumanok some cry of cleaner things;\\n'\n 'And the Green Carnation withered, as in forest fires that pass,\\n'\n 'Roared in the wind of all the world ten million leaves of grass;\\n'\n 'Or sane and sweet and sudden as a bird sings in the rain--\\n'\n 'Truth out of Tusitala spoke and pleasure out of pain.',\n 'Yea, cool and clear and sudden as a bird sings in the grey,\\n'\n 'Dunedin to Samoa spoke, and darkness unto day.']\n"
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is difficult because period is used to mark abbreviations, and some periods simultaneously mark an abbreviation and terminate a sentence, as often happens with acronyms like U.S.A.\n",
    "\n",
    "### 3.8.2 Word Segmentation\n",
    "\n",
    "For some writing systems, tokenizing text is made more difficult by the fact that there is no visual representation of word boundaries. For example, in Chinese, the three-character string: 爱国人 (ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\") could be tokenized as 爱国 / 人, \"country-loving person\" or as 爱 / 国人, \"love country-person.\"\n",
    "\n",
    "A similar problem arises in the processing of spoken language, where the hearer must segment a continuous speech stream into individual words. A particularly challenging version of this problem arises when we don't know the words in advance. This is the problem faced by a language learner, such as a child hearing utterances from a parent. Consider the following artificial example, where word boundaries have been removed:\n",
    "\n",
    "(1)\t\t\n",
    "\n",
    "a.\t\tdoyouseethekitty\n",
    "\n",
    "b.\t\tseethedoggy\n",
    "\n",
    "c.\t\tdoyoulikethekitty\n",
    "\n",
    "d.\t\tlikethedoggy\n",
    "\n",
    "Our first challenge is simply to represent the problem: we need to find a way to separate text content from the segmentation. We can do this by annotating each character with a boolean value to indicate whether or not a word-break appears after the character (an idea that will be used heavily for \"chunking\" in 7.). Let's assume that the learner is given the utterance breaks, since these often correspond to extended pauses. Here is a possible representation, including the initial and target segmentations:\n",
    "\n",
    "Observe that the segmentation strings consist of zeros and ones. They are one character shorter than the source text, since a text of length n can only be broken up in n-1 places. The segment() function in 3.7 demonstrates that we can get back to the original segmented text from the below representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n--------------------------------------------------\n['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n"
    }
   ],
   "source": [
    "print(segment(text, seg1))\n",
    "print(\"-\"*50)\n",
    "print(segment(text, seg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe",
   "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}