{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.\tShort Essay Responses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select one career or industry that makes use of applied NLP.\n",
    "\n",
    "One industry where NLP is applied extensively is eCommerce.\n",
    "\n",
    "### 1A. Explain generally how that field  or career utilizes NLP.\n",
    "\n",
    "A couple of places where NLP is being used in eCommerce is for **comparison shopping** and for **extracting product features** from the product description. \n",
    "\n",
    "**Comparison Shopping**: When a customer comes to an eCommerce website, they may type a general description of the product that they are looking for. By presenting only the most relevant products to the customer, the eCommerce company can potentially lock in the customer's decision faster and close the sale of the product. If the customer has to wade through irrelevant search results to get to what they want instead, the company might end up losing this customer. NLP can be used to aid this comparison process by finding the closest matching products to the users search query.\n",
    "\n",
    "**Extracting Product Features from Product Description**: In today's world where customers may have short attention span, vendors selling products usually misuse the product description field where they not only add the product description but a lot of the product features as well. This is done so that they can catch the customers attention faster. However, we can use NLP to extract the feature information from the product description to make an accurate catalog of the products being sold and this information can also use used to feed the feature filters that show up on the comparison shopping page. \n",
    "\n",
    "\n",
    "### 1B. Explain at least some methods of NLP that are very likely to be used in the career or industry you selected.\n",
    "\n",
    "**Comparison Shopping**: One of the NLP methods that this might use is sentence similarity. Here, sentence refers to the product description. We may choose to train a model (to get the word vectors or embeddings) using the vocabulary of the products in our eCommerce website. Using these word vectors, we can compute how similar the product descriptions are to what the user has typed in the search box. While there may be various methods to extract the single \"vector\" corresponding to the product description, we would start with a very basic method of taking the average of the word vectors in the product description and the average of the word vectors in the users search and compare the 2 to find the closest matching products.\n",
    "\n",
    "**Extracting Product Features from Product Description**: The method most likely to be used in this case could be sentence segmentation. Before we extract the exact product features out of the product description, we would want to break up the product descriptions into the sections that describe each individual feature. For this we may have to first create a training dataset that indicates when a feature description starts and when it ends in the product description. We can then train a model to recognize this and this trained model can eventually be used to segment the product description into the constituent features of the product.\n",
    "\n",
    "### 1C. Give at least one specific example of a use case for NLP within the chosen field, and explain how the problem or situation is (or could be) improved by applying NLP.\n",
    "\n",
    "Let's look at the example of **Comparison Shopping** to see how this could be improved by using NLP. Let's say that the customer is specifically searching for an older version (version 1) of a camera on the eCommerce website because it is cheaper than the latest version (version 2). The eCommerce company may have many vendors selling both versions of this camera. However, the eCommerce company may not specifically have a field for \"product version\" for this product so the vendors may choose to add the product version to the description section itself. However the word vector for version 1 may be different from the word vector for version 2 so when we do the \"product similarity\" comparison using the word vectors, we will most likely end up presenting version 1 as the top search results to the customer since that would be closest to the users search query. Alternately, if we had not used this method, we might have presented version 2 as the top result (since that is what more customer might be searching for at this point in time), but that would not have been relevant to this customer. In the worst case, if version 1 did not show up on the first page of the search, the customer may have gone to a different eCommerce website thinking that version 1 is not offered at this website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose one of the “trade-offs” in NLP that was covered in the asynchronous materials for this course.\n",
    "\n",
    "In terms of trade-offs in NLP, the one that resonates with me is Feature Learning vs. Feature Engineering. \n",
    "\n",
    "### 2A. Explain the trade-off in general terms. Define the two choices.\n",
    "\n",
    "In simple terms, **Feature Learning** means that using (a lot of) data, train a machine learning or deep learning algorithm to automatically figure out the “important features” needed to get the best metrics possible. In this approach, we rely on the machine to figure out (mine) the right features from a large corpus of data. \n",
    "\n",
    "On the contrary, **Feature Engineering** relies on a Subject Matter Expert (SME) to define the right set of features to get the best metric for the problem we are trying to solve. This method relies on the SMEs expertise and intuition to define these features.\n",
    "\n",
    "### 2B. Explain the benefits and weaknesses of each side of the trade-off.  Include at least one benefit and one weakness of each.\n",
    "\n",
    "In terms of pros and cons of both approaches, **Feature Learning** is useful when we have a lot of labelled data already available in which case, we can feed this into our algorithm quickly (maybe with a little bit of data cleaning and EDA). This method requires minimal interaction with the SMEs who may otherwise be busy with their \"day\" jobs and may not be available to help if needed. On the other hand, if large amounts of labelled data are not available, the algorithms will most likely fail to recognize the right set of \"features\" from the data. Also, this approach tends to alienate the SMEs since they feel that the decisions are being made by a \"black box\" algorithm and these decisions may not always be explainable or make sense to these experts.\n",
    "\n",
    "On the contrary, the pros of **Feature Engineering** by a SME is that the experts feel somewhat in control and have a vested interest in the success of the work. This can lead to better buy in from management teams. In addition, in many cases, large amounts of labelled may not be available and in these cases, hand engineering of the important features by an SME would be a logical choice. The cons of feature engineering that we must find a cooperative SME who is willing to work with the data scientist/machine learning engineer in order to define these features.  \n",
    "\n",
    "### 2C. Describe a work-situation that would make one of the choices in the trade-off much better, in terms of practical outcomes for you and your stakeholders on a project.\n",
    "\n",
    "The real way to make the decision of whether to use Feature Learning or Feature Engineering depends on the stakeholders and the users of the models. The domain that I work in is Analog Circuit Design. This field is considered highly specialized and Subject Matter Experts (SMEs) consider this to be more of an \"art\" form than science. Hence any notion of \"automation\" is frowned upon by a large section of the population. Because of this, \"Feature Engineering\" by a SME might be more appropriate. In addition it will likely lead to better accuracy as well as illustrated by the examples below.\n",
    "\n",
    "**Example 1**: One of the use cases is to look at different blocks of a design and try to classify what kind of block it is. Unfortunately, although there are a lot of designs (data) available for analysis, there is not a lot of labelled data available. Hence the first task is to create labelled data. One way to do this is to look at the name of the block and take a \"best guess\" using regular expressions. This may not need us to involve the SME. But this can be error prone since the names of the blocks are free form text. For example a block containing the letters \"AMP\" might most like be an amplifier, but a block containing the word \"RAMP\" (which also contains \"AMP\") will most likely be an oscillator. Without the involvement of SMEs, this kind of \"features\" may not be captured in the regular expressions and may lead to lower accuracy. \n",
    "\n",
    "**Example 2**: Another problem where involving the SME might be helpful is that of comparing 2 design blocks (which can be represented as graphs). Although not directly NLP, this method draws heavily from the NLP literature by using methods like node2vec, sub2vec, graph2vec. These methods are analogous to word2vec and are trained using methods that were originally proposed in NLP literature such as skipgram and Continuous Bag of Words methods. When using these methods though, it is importaht to capture the design information appropriately in the graphs. This includes nodes (equivalent to the \"words\" in our vocabulary in NLP) and edges (equivalent to the relationships between words in NLP). This process is very domain specific and would not be successful without the involvement of SMEs to represent the design in the form of an appropriate graph. Once the graphs have been appropriately defined, we can rely on the \"feature learning\" approach to assign the appropriate embedding to the blocks in the design so that we can compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. NLP Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data Prep\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "## Layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, TimeDistributed, Bidirectional, Lambda\n",
    "from tensorflow.keras.layers import SimpleRNN as RNN\n",
    "from tensorflow.keras.layers import LSTM as LSTM\n",
    "from tensorflow.keras.layers import GRU as GRU\n",
    "\n",
    "## Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I have a vocabulary of 10 words assigned the following indexes (in a dictionary):\n",
    "\n",
    "```\n",
    "{\n",
    "    \"the\":  0, \"quick\":  1, \"brown\": 2, \"fox\": 3, \"jumped\": 4,\n",
    "    \"over\": 5, \"fence\": 6, \"under\": 7, \"car\" : 8, \"did\": 9\n",
    "}\n",
    "```\n",
    "I have a network that classifies a sentence as a question or a statement.  0 means statement, 1 indicates a question. I give you the following code as the network:\n",
    "\n",
    "```\n",
    "# truncate and pad input sequences\n",
    "max_sent_length = 8\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_sent_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_sent_length)\n",
    "\n",
    "embedding_vec_length = 75\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_sent_length))\n",
    "model.add(LSTM(115, return_sequences=True))\n",
    "model.add(RNN(95))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Draw/Make a diagram of this network using an input sequence of “the car jumped over the fence ”\n",
    "\n",
    "Assumptions:\n",
    "The sequence tokens are words, split by whitespace.\n",
    "You may label a cell by its type—there is no need to show the inner connections of the LSTM cell.  (A quick reminder—LSTM has 4 sets of gates/weights, but all those gates/weights have the same size matrix—that size is what I am after !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'quick': 1,\n",
       " 'brown': 2,\n",
       " 'fox': 3,\n",
       " 'jumped': 4,\n",
       " 'over': 5,\n",
       " 'fence': 6,\n",
       " 'under': 7,\n",
       " 'car': 8,\n",
       " 'did': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {\n",
    "    \"the\":  0, \"quick\":  1, \"brown\": 2, \"fox\": 3, \"jumped\": 4,\n",
    "    \"over\": 5, \"fence\": 6, \"under\": 7, \"car\" : 8, \"did\": 9\n",
    "}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one thing missing in this setting are the tokens for PAD, START and UNK. We will add this to the beginning of the dictionary and move the keys for all other words by 3 to account for this. This also answers question 6 to some extent, but we will elaborate later again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<UNK>': 2,\n",
       " 'the': 3,\n",
       " 'quick': 4,\n",
       " 'brown': 5,\n",
       " 'fox': 6,\n",
       " 'jumped': 7,\n",
       " 'over': 8,\n",
       " 'fence': 9,\n",
       " 'under': 10,\n",
       " 'car': 11,\n",
       " 'did': 12}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = {}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "for index, word in enumerate(vocab):\n",
    "    word_index[word] = index+3\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = len(word_index)\n",
    "max_sent_length = 8\n",
    "\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_sent_length)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_sent_length)\n",
    "\n",
    "embedding_vector_length = 75\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_sent_length))\n",
    "model.add(LSTM(115, return_sequences=True))\n",
    "model.add(RNN(95))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8, 75)             975       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8, 115)            87860     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 95)                20045     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 96        \n",
      "=================================================================\n",
      "Total params: 108,976\n",
      "Trainable params: 108,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label each block and step by input/sequence step. Compute the dimensions of the weight for all steps. All inputs must be labeled by dimension. Include your original word ENCODING (notice not vector!) as input. You may omit bias.\n",
    "\n",
    "**INPUT:** The input to the network will be a single sequence of 8 integers (representing the sequence of length 8). The sequence will be padded if needed (as shown in this example -- the first 2 entries are padded with 0).\n",
    "\n",
    "**EMBEDDING MATRIX:** Internally, these integers are One Hot Encoded (OHE) before feeding to the embedding matrix. The length of the OHE vector is the size of the vocabulary. This indicates the row of the embedding matrix that must be pulled for each word in the sequence.\n",
    "\n",
    "The size of the embedding matrix is equal to the vocabulary size * length of the embedding vector (i.e. since there is an embedding vector for each word in the vocabulary)\n",
    "\n",
    "The OHE input is multiplied with the embedding matrix to produce an output of size sequence length * length of the embedding vector. This is basically the embeddings corresponding to each word in the sequence.\n",
    "\n",
    "**LSTM Layer:** The input to the LSTM layer comes from the Embedding Layer (sequence length x embedding vector length). In addition to this, the output of the LSTM is fed back to the LSTM as well (appended to the embedding vector). Hence the size of the LSTM Weight Matrix is (embedding vector length + number of LSTM neurons) x number of LSTM neurons. There are 4 such matrices. Since return sequence is True for the LSTM layer, the output of each time step is returned and this is fed into the RNN layer. The size of each time step output from the LSTM layer is 1 x number if LSTM neurons (and there will be 8 such outputs - one for each time step in the sequence).\n",
    "\n",
    "**RNN Layer:** The input to the RNN layer comes from the LSTM Layer (sequence length x number of LSTM neurons). In addition to this, the output of the RNN is fed back to the RNN as well (appended to the input). Hence the size of the RNN Weight Matrix is (number of LSTM neurons + number of RNN neurons) x number of RNN neurons. There is only 1 such matrix in the RNN. Since return sequence is False for the RNN layer, the output of only the last time step is returned and this is fed into the dense layer. The size of this output from the RNN layer is 1 x number if RNN neurons.\n",
    "\n",
    "**Dense Layer:** The dense layer is straightforward and has the same number of neurons as the RNN layer. There is only 1 output of this dense layer that produces an output 1 for a \"question\" and 0 for a \"statement\".\n",
    "\n",
    "### View 1\n",
    "\n",
    "**NOTE: If the image does not load, please refer to 2p1.JPG in this folder.**\n",
    "\n",
    "<img src=\"2p1.JPG\">\n",
    "\n",
    "### Alternate View\n",
    "\n",
    "**NOTE: If the image does not load, please refer to 2p1_alt.JPG in this folder.**\n",
    "\n",
    "<img src=\"2p1_alt.JPG\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write the initial vector form of the input sequence using only 1s and 0s\n",
    "\n",
    "The initial vectors of the input sequence (One Hot Encoded) are shown below. There is a '1' in the index corresponding to the location of the word in the dictionary and a 0 at all other indices. \n",
    "\n",
    "**NOTE: If the image does not load, please refer to 2p2.JPG in this folder.**\n",
    "\n",
    "<img src=\"2p2.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the average GloVe Word Vector of your the input sequence (Spacy uses Glove vectors!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 300)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"the car jumped over the fence\")\n",
    "vectors = []\n",
    "for token in doc:\n",
    "    # print(token.text, token.pos_, token.dep_, token.vector.size)\n",
    "    vectors.append(token.vector)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Average GloVe vector: (300,)\n",
      "The average GloVe vector: [ 1.02246672e-01  6.65950105e-02 -1.49527833e-01  3.49901654e-02\n",
      "  2.13985667e-01 -7.65089318e-02 -2.45101675e-01  7.83283338e-02\n",
      "  2.09699962e-02  2.41605020e+00 -1.05761170e-01  5.89891970e-02\n",
      "  3.98833267e-02 -6.72252774e-02 -2.43400812e-01 -1.21773286e-02\n",
      " -7.25516751e-02  1.03658164e+00 -2.32895855e-02 -1.55521646e-01\n",
      "  2.29424983e-02 -8.47426429e-02  2.09339336e-01  1.32689821e-02\n",
      "  1.60996635e-02 -1.53346330e-01 -8.59160051e-02 -1.78890824e-01\n",
      " -9.34920013e-02 -2.61886623e-02  4.54970933e-02  2.78918356e-01\n",
      " -1.12586327e-01  2.80501485e-01  1.23955004e-01 -1.41709670e-01\n",
      " -4.83516753e-02  1.34987980e-02  1.60396993e-02 -3.77281681e-02\n",
      "  2.11798310e-01  4.15756665e-02 -5.41546196e-02 -6.71449974e-02\n",
      "  8.97116885e-02 -4.67299968e-02 -2.15548679e-01 -3.80335063e-01\n",
      "  6.24183305e-02  1.80273965e-01 -4.30923253e-02 -6.58916831e-02\n",
      " -1.00524008e-01  1.68676674e-01 -1.23110332e-01  3.10916658e-02\n",
      "  4.90240008e-02 -6.93003386e-02 -5.66300005e-02 -6.21146746e-02\n",
      "  1.06594689e-01  1.92563340e-01 -2.24631652e-02  2.88225979e-01\n",
      "  6.67833313e-02  3.77099998e-02 -3.47989984e-02 -1.93191484e-01\n",
      "  1.17071338e-01  1.61466673e-01 -1.18047833e-01 -1.62384976e-02\n",
      "  3.14163327e-01  2.70906657e-01 -4.48926575e-02 -4.22549956e-02\n",
      "  1.23116501e-01 -2.96609342e-01 -1.39579833e-01  8.14026669e-02\n",
      "  1.10720165e-01  3.54271650e-01  1.22986631e-02 -3.63661647e-02\n",
      "  1.21970713e-01  1.98808834e-01  7.96278417e-02  2.64610112e-01\n",
      "  1.20267332e-01  1.64494321e-01 -6.51488379e-02 -3.08013353e-02\n",
      "  6.57665059e-02 -1.18793339e-01 -1.21375002e-01  3.01500154e-03\n",
      "  4.15491648e-02  1.44447833e-01  3.20313334e-01 -2.33143326e-02\n",
      "  1.13832675e-01  5.34916669e-02 -2.11709991e-01 -1.60306171e-01\n",
      "  3.01343352e-02 -6.81866646e-01  1.71360001e-01  3.60621698e-02\n",
      " -1.91083346e-02 -8.79466757e-02  9.86851528e-02 -9.48226675e-02\n",
      "  3.48386019e-01 -1.45902663e-01  2.00064316e-01  6.77258372e-02\n",
      " -1.20133413e-02  6.13599978e-02  1.19159997e-01  3.63789983e-02\n",
      "  2.38830000e-02 -1.61527157e-01 -1.02036051e-01 -3.16459656e-01\n",
      " -2.49000326e-01 -2.51293313e-02 -5.27970009e-02 -1.01913333e-01\n",
      " -4.52106707e-02  4.09161337e-02 -2.05126658e-01  1.94621012e-01\n",
      " -3.07096660e-01  6.72188327e-02  6.60986602e-02 -1.50133327e-01\n",
      "  2.80357990e-02  2.23969996e-01  2.87990030e-02 -9.97816697e-02\n",
      " -1.43598330e+00 -5.37484922e-02  1.56250343e-01  9.46183223e-03\n",
      "  1.09009333e-01 -3.36200255e-03 -4.48800027e-02 -3.68850045e-02\n",
      "  2.72751659e-01  1.41264990e-01 -2.70038337e-01 -7.61556700e-02\n",
      "  1.36434823e-01 -8.98864940e-02 -8.02138373e-02 -2.03154027e-01\n",
      "  1.20266654e-01  3.84640008e-01  2.84733325e-02  6.68138266e-02\n",
      " -1.45843834e-01  8.97001699e-02 -1.28053337e-01 -1.00520827e-01\n",
      " -4.73484956e-02 -1.14998333e-01  6.95599988e-02 -3.95846665e-02\n",
      "  1.30314782e-01  1.50368169e-01  1.25569835e-01  5.13156652e-02\n",
      " -1.71585009e-01 -7.21875057e-02 -2.73713320e-01 -3.97153310e-02\n",
      " -5.06145023e-02  2.36549005e-01  2.91716699e-02 -1.71874329e-01\n",
      "  1.51221335e-01 -1.54766232e-01 -2.89241672e-01 -1.96124002e-01\n",
      " -5.10866335e-03 -1.69838175e-01 -2.17521656e-02 -1.67769507e-01\n",
      "  3.57716642e-02 -4.23661657e-02  2.57701337e-01  7.14846626e-02\n",
      " -7.01555312e-02  1.91963330e-01 -1.32479936e-01  1.03625663e-01\n",
      "  1.56697720e-01 -2.53173355e-02 -2.86182493e-01  7.02083623e-03\n",
      " -4.01316732e-02 -2.23066639e-02 -2.45521680e-01  6.72115162e-02\n",
      "  2.54228026e-01  5.75384684e-02  1.05767667e-01 -1.16228998e-01\n",
      "  3.61099951e-02 -1.28049990e-02 -5.97448312e-02  1.28031671e-02\n",
      "  3.52631658e-02  6.83058724e-02  8.96621570e-02  1.96179509e-01\n",
      " -2.67955840e-01  2.30717853e-01 -7.25523308e-02 -1.13187671e-01\n",
      "  2.49129981e-02 -1.05219662e-01 -1.22388333e-01 -1.36076659e-02\n",
      " -2.43960977e-01 -3.24010491e-01  6.23329252e-04  1.39413312e-01\n",
      " -2.04223339e-02  7.26416633e-02 -9.96336341e-04  7.41533339e-02\n",
      " -9.62996706e-02 -3.63566726e-02 -6.97279871e-02 -7.91983381e-02\n",
      "  9.13333297e-02  1.53121501e-01  1.03963353e-02  7.81561658e-02\n",
      " -8.12756717e-02 -3.50351669e-02  4.68028337e-02  1.41970322e-01\n",
      " -5.43585867e-02 -3.55706662e-01  1.19163997e-01 -1.45466819e-01\n",
      " -8.59850049e-02 -4.72178273e-02 -1.90006986e-01 -6.41948357e-02\n",
      "  1.46570489e-01  4.02814709e-02  5.15481681e-02  3.15691680e-01\n",
      "  1.67254999e-01  1.07822001e-01  1.07833361e-02 -1.48848668e-01\n",
      "  2.58671850e-01  5.72443306e-02  8.61733258e-02 -3.16029310e-01\n",
      "  2.91208327e-01 -2.62440503e-01 -7.26199970e-02 -1.19893990e-01\n",
      " -4.26665060e-02 -1.96394488e-01  2.56954014e-01  7.53486678e-02\n",
      "  4.77932766e-03 -1.09933324e-01  8.91481638e-02 -3.66953202e-02\n",
      "  7.48030022e-02  1.68176994e-01  2.46623307e-02 -3.43943350e-02\n",
      " -7.04811588e-02  3.51395011e-02 -1.66051671e-01  1.31367818e-01\n",
      "  6.32465035e-02 -4.31626625e-02 -4.79533756e-03  1.37216842e-03\n",
      " -4.09579948e-02 -5.88233359e-02 -5.01784980e-01 -1.69103339e-01\n",
      "  2.81081647e-01 -1.91766188e-01 -6.20938353e-02 -9.28726569e-02\n",
      "  1.14023328e-01  1.77116826e-01  9.68199968e-02  6.52955025e-02]\n"
     ]
    }
   ],
   "source": [
    "mean_glove = np.mean(vectors, axis = 0)\n",
    "print(f\"Shape of the Average GloVe vector: {mean_glove.shape}\")\n",
    "print(f\"The average GloVe vector: {mean_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternately, we could have used the .vector method to get the mean vector\n",
    "all(doc.vector == mean_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find the nearest word (in the above dictionary) to answer #3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarty(word1: np.array, word2: np.array, numpy=True, scipy=False, verbose: bool = True) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Function computes the similarity between 2 word vectors.\n",
    "    Computation is done using 2 methods: (1) using Scipy formula and (2) using Spacy/Gensim equivalent formula created with Numpy\n",
    "    :param: word1: Word Vector for the 1st word\n",
    "    :type: word1 np.array\n",
    "    :param: word2: Word Vector for the 2nd word\n",
    "    :type: word2 np.array\n",
    "    :param numpy Whether to print the distance calculation using numpy (Default: True)\n",
    "    :type numpy Bool\n",
    "    :param scipy Whether to print the distance calculation using scipy (Default: False)\n",
    "    :type numpy Bool\n",
    "    :rtype Optional[float]\n",
    "    \"\"\"\n",
    "    cosine_similarity = None\n",
    "    if scipy:\n",
    "        # NOTE: that scipy uses distance, hence in order to calculate the similarity, we need to take 1 - distance\n",
    "        cosine_distance_scipy = spatial.distance.cosine(word1, word2)  ## Scipy\n",
    "        if verbose:\n",
    "            print(f\"Cosine similarity using scipy (default: computes distance, i.e. less distance is more similar): {cosine_distance_scipy}\")\n",
    "        cosine_similarity = 1 - cosine_distance_scipy  ## Scipy\n",
    "        if verbose:\n",
    "            print(f\"Cosine Similarity using scipy (corected from distance to actual similarity): {cosine_similarity}\")\n",
    "    if numpy:\n",
    "        cosine_similarity = np.dot(word1, word2)/(norm(word1)*norm(word2)) ## Manual\n",
    "        if verbose:\n",
    "            print(f\"Cosine Similarity using numpy (same formula as Gensim and SpaCy): {cosine_similarity}\")  \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the quick brown fox jumped over fence under car did"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_doc = nlp(\" \".join(list(vocab.keys())))\n",
    "vocab_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: the --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.7533266544342041\n",
      "\n",
      "Word: quick --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.46778127551078796\n",
      "\n",
      "Word: brown --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.2831871509552002\n",
      "\n",
      "Word: fox --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.310445100069046\n",
      "\n",
      "Word: jumped --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.650532066822052\n",
      "\n",
      "Word: over --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.7485372424125671\n",
      "\n",
      "Word: fence --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.6448632478713989\n",
      "\n",
      "Word: under --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.491545170545578\n",
      "\n",
      "Word: car --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.6525694727897644\n",
      "\n",
      "Word: did --> \n",
      "Cosine Similarity using numpy (same formula as Gensim and SpaCy): 0.5797324776649475\n",
      "\n",
      "\n",
      "[0.75332665, 0.46778128, 0.28318715, 0.3104451, 0.65053207, 0.74853724, 0.64486325, 0.49154517, 0.6525695, 0.5797325]\n"
     ]
    }
   ],
   "source": [
    "similarities = []\n",
    "\n",
    "for token in vocab_doc:\n",
    "    print(f\"\\nWord: {token.text} --> \")\n",
    "    similarities.append(compute_similarty(token.vector, mean_glove, verbose=True))\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(similarities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest word in the original dictionary to the mean GloVe vector for the sentence is: 'the'\n"
     ]
    }
   ],
   "source": [
    "index = similarities.index(max(similarities))  # Index of nearest word\n",
    "print(f\"The closest word in the original dictionary to the mean GloVe vector for the sentence is: '{vocab_doc[index]}'\") # Nearest word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is the difference between the W(weight) matrix of the first LSTM sequence at time/sequence 0 and at time/sequence 5.  How do you know this?  \n",
    "\n",
    "\n",
    "During the forward pass the training process, the weight matrix for the LSTM is the same at time step 0 as it is for time step 5. This is clear when we look at the LSTM in a \"rolled\" form. Usually, the LSTM is drawn out in the \"unrolled\" form which makes us believe that there is a separate value of the weight matrix at each time step. However in the \"rolled\" form, we can clearly see that the output of the LSTM is fed back to the same LSTM as an input. \n",
    "\n",
    "Another way we know this is that if we calculate the number of parameters in the weight matrix, we can see that it does not depend on the sequence length. It is only dependent on the number of neurons and the size of the input vector at each step. We can see this from the analysis below where a network with sequence length 8 and sequence length 80 both have the same number of parameters (weights) for the LSTM and RNN layers. Since the Weight is independent of the step size (or sequence length), we know that the weight matrix does not depend on the time step in the sequence.\n",
    "\n",
    "Since the weight at time step 0 is the same as time step 5, the difference between the weight matrices will be 0.\n",
    "\n",
    "The only time there will be a difference between the weights at time step 5 and time step 0 is during the back propagation step when the weights are being updated but this is a transient (temporary) state. Once back propagation is over and the weights have been updated, the same weights are used for all time steps in the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 8, 75)             975       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8, 115)            87860     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 95)                20045     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 96        \n",
      "=================================================================\n",
      "Total params: 108,976\n",
      "Trainable params: 108,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# With Sequence length of 8\n",
    "top_words = len(word_index)\n",
    "max_sent_length = 8\n",
    "\n",
    "embedding_vector_length = 75\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_sent_length))\n",
    "model.add(LSTM(115, return_sequences=True))\n",
    "model.add(RNN(95))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 75)            975       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 80, 115)           87860     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 95)                20045     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 96        \n",
      "=================================================================\n",
      "Total params: 108,976\n",
      "Trainable params: 108,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# With Sequence length of 80\n",
    "top_words = len(word_index)\n",
    "max_sent_length = 80\n",
    "\n",
    "embedding_vector_length = 75\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_sent_length))\n",
    "model.add(LSTM(115, return_sequences=True))\n",
    "model.add(RNN(95))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is missing in the above code—something important is not determined and based on that, there are some minor adjustments or additions that need to be made.  Make a logical determination of what that missing piece of info should be based on the info given here and what additions or adjustments are necessary. \n",
    "\n",
    "We already discussed this earlier that the missing piece in this were the tags for PAD, START and UNK (although for this illustration only the PAD was important, START and UNK could be important for other applications). Because we choose to add these tags to the beginning of our vocabulary (PAD = 0, START = 1, UNK = 2), we had to shift all the other words down by 3 in the vocabulary index. This is the usual convention since the padding function has 0 as the default value.\n",
    "\n",
    "Alternately, we could have chosen to add these 3 tokens at the end of the vocabulary. In that case, we would not have had to shift the other words down, but we would have had to modify the padding to indicate the position of the PAD in our vocabulary. For example if PAD was at index 10, the new code would look like this:\n",
    "\n",
    "```\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_sent_length, value=10)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_sent_length, value=10)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)",
   "language": "python",
   "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
