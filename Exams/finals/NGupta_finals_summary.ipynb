{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGupta_finals_summary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWW7F20gfhpkxs+a/eFSSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngupta23/ds7337_nlp/blob/master/Exams/finals/NGupta_finals_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n3-OWBKoMPV",
        "colab_type": "text"
      },
      "source": [
        "# PART I\n",
        "Given the GOP twitter dataset (a dataset of tweets from 2012 with 3 sentiments‚Äîsee the attached file):\n",
        "\n",
        "1. Build a model  the predict the sentiment (column ‚ÄúSentiment‚Äù) of the tweet based on a <ins>sequence of characters</ins> and a second model based on a <ins>sequence of bi-grams</ins>  (2-letter sequences). Please include ALL the code you used to develop this model .\n",
        "\n",
        "2. What is the vector  you learned for the following emoji - üòÇ. If you have trouble figuring out which face it is, the total count in the data set is 95. Send your model to rslater@smu.edu (Probably to big to submit to 2DS).\n",
        "```python\n",
        "Save the model\n",
        "model.save('path_to_my_model.h5')\n",
        "```\n",
        "\n",
        "3. What is the most similar character for the above emoji\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDBQrrtwozGz",
        "colab_type": "text"
      },
      "source": [
        "## Answer\n",
        "\n",
        "The development process for the 2 models can be found in the files: \n",
        "1. **Using sequence of characters**: `NGupta_question1_unigram.ipynb`\n",
        "2. **Using sequence of bi-grams**: `NGupta_question1_bigram.ipynb`\n",
        "\n",
        "These 2 models are also available in `unigram_model.h5` and `bigram_model.h5` respectively.\n",
        "\n",
        "The vector for the emoji and its most similar character is shown at the end of the unigram model file `NGupta_question1_unigram.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-KyGB23oSur",
        "colab_type": "text"
      },
      "source": [
        "# PART II\n",
        "\n",
        "Build a Universal Sentence Encoder Model and an RNN model for the following data set: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip (first column is the polarity 0:negative, 4:positive).\n",
        "\n",
        "Compare the results of the two in terms of time and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr0iV8bNqV09",
        "colab_type": "text"
      },
      "source": [
        "## Answer\n",
        "\n",
        "The development process for the 2 models can be found in the files: \n",
        "1. **Universal Sentence Encoder**: `NGupta_question2_USE.ipynb`\n",
        "2. **LSTM**: `NGupta_question2_LSTM.ipynb`\n",
        "\n",
        "| Metric                         | Universal Sentence Encoder (USE)                                                                                                         | LSTM                                                                                                                              |\n",
        "|--------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Tokenizing                     | Not needed explicitly                                                                                                                    | Needed Explicitly. Can be slow for larger datasets.<br/>Took ~45s for train and validation datasets in this case. |\n",
        "| Number of Trainable Parameters | ~51K (without retraining USE)<br/> ~256M (with retraining, not used)                                                                          | ~53M                                                                                                                              |\n",
        "| Training Time                  | Approximately 95s per epoch with Batch Size = 4096. <br />   Does not change much is a smaller batch size of 1024<br>(~97s per epoch, not shown).  | Approximately 150s with Batch Size = 4096. <br/>Much slower with smaller batch size of 1024<br/> (~475s per epoch, not shown)             |\n",
        "| Accuracy                       | Validation Accuracy ~ 80%                                                                                                                | Validation Accuracy ~ 80%                                                                             "
      ]
    }
  ]
}