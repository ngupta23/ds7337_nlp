{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe",
      "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "32WrJ5wFWDbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "d4238038-544f-4f2a-c26c-dbd1b6f886d5",
        "tags": []
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: bert-for-tf2 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.14.4)\nRequirement already satisfied: py-params>=0.9.6 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.9.7)\nRequirement already satisfied: params-flow>=0.8.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.8.2)\nRequirement already satisfied: numpy in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\nRequirement already satisfied: tqdm in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.46.0)\nRequirement already satisfied: sentencepiece in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.1.91)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS8vnMp-WNrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model \n",
        "import bert\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rU1nCigWVPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",trainable=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfbI-M6UWaAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69b66bd3-e9bf-410b-dec6-1efc492b5b56",
        "tags": []
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Vocab size: 30522\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM6SsEh7XG3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 50  # Your choice here.\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"segment_ids\")\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "classifier1 = tf.keras.layers.Dense(100,activation='sigmoid')(pooled_output)\n",
        "classifier2 = tf.keras.layers.Dense(1,activation='sigmoid')(classifier1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nunxWgbFXPAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[classifier2])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB1qibbGXX-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "88c2806d-271f-45b6-caa0-d7eebe222e0b",
        "tags": []
      },
      "source": [
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 50)]         0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 50)]         0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 50)]         0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 100)          76900       keras_layer[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            101         dense[0][0]                      \n==================================================================================================\nTotal params: 109,559,242\nTrainable params: 109,559,241\nNon-trainable params: 1\n__________________________________________________________________________________________________\nFailed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr_Z0JcPXiA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ca50427a-5a1b-4adb-ef57-a48581afc2d1",
        "tags": []
      },
      "source": [
        "tokens = tokenizer.tokenize(\"cyclohexane was on the test\")\n",
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['cy', '##cl', '##oh', '##ex', '##ane', 'was', 'on', 'the', 'test']\n[22330, 20464, 11631, 10288, 7231, 2001, 2006, 1996, 3231]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR-q3r3NXpBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fafbb154-8b4e-4e4b-8c0d-f719d0d21dc8"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[101, 102]"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyZufuuX3sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(s):\n",
        "   tokens = [i for i in list(tokenizer.tokenize(s))]\n",
        "   tokens = tokens[1:]\n",
        "   return np.array(tokenizer.convert_tokens_to_ids(tokens))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2kYh87qUEr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKW22ujzH8w0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(x_data, tokenizer, seq_len):\n",
        "  num_examples = len(x_data)\n",
        "  \n",
        "  data = []\n",
        "  for i in x_data:\n",
        "    tmp = encode_sentence(i)\n",
        "    if len(tmp) > seq_len-1:\n",
        "      tmp = tmp[0:seq_len-1]\n",
        "    data.append(tmp)\n",
        "  sentence = tf.ragged.constant(data)\n",
        " \n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence], axis=-1)\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s1 = tf.zeros_like(sentence)\n",
        "  input_type_ids = tf.concat(\n",
        "      [type_cls, type_s1], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'segment_ids': input_type_ids}\n",
        "\n",
        "  return inputs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKH1hXFgX6a1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJhOcB6qYdsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "716aeab5-638f-4032-f26c-5e721b97e4e3",
        "tags": []
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "reverse_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i,\"?\") for i in training_data[0]] )\n",
        "print(decoded[:5]) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<STAR\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIUwn8luZHCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "d814b42e-97d8-4508-ee9b-98ed4f86f184",
        "tags": []
      },
      "source": [
        "tokens = tokenizer.tokenize(decoded)\n",
        "print(tokens)\n",
        "encode_sentence(decoded)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['<', 'start', '>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', 'everyone', \"'\", 's', 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '<', 'un', '##k', '>', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '<', 'un', '##k', '>', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '<', 'un', '##k', '>', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '<', 'un', '##k', '>', 'to', 'the', 'two', 'little', 'boy', \"'\", 's', 'that', 'played', 'the', '<', 'un', '##k', '>', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '<', 'un', '##k', '>', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', 'don', \"'\", 't', 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', 'someone', \"'\", 's', 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([ 2707,  1028,  2023,  2143,  2001,  2074,  8235,  9179,  3295,\n       17363,  2466,  3257,  3071,  1005,  1055,  2428, 10897,  1996,\n        2112,  2027,  2209,  1998,  2017,  2071,  2074,  5674,  2108,\n        2045,  2728,  1026,  4895,  2243,  1028,  2003,  2019,  6429,\n        3364,  1998,  2085,  1996,  2168,  2108,  2472,  1026,  4895,\n        2243,  1028,  2269,  2234,  2013,  1996,  2168,  4104,  2479,\n        2004,  2870,  2061,  1045,  3866,  1996,  2755,  2045,  2001,\n        1037,  2613,  4434,  2007,  2023,  2143,  1996, 25591, 12629,\n        2802,  1996,  2143,  2020,  2307,  2009,  2001,  2074,  8235,\n        2061,  2172,  2008,  1045,  4149,  1996,  2143,  2004,  2574,\n        2004,  2009,  2001,  2207,  2005,  1026,  4895,  2243,  1028,\n        1998,  2052, 16755,  2009,  2000,  3071,  2000,  3422,  1998,\n        1996,  4875,  5645,  2001,  6429,  2428,  6639,  2012,  1996,\n        2203,  2009,  2001,  2061,  6517,  1998,  2017,  2113,  2054,\n        2027,  2360,  2065,  2017,  5390,  2012,  1037,  2143,  2009,\n        2442,  2031,  2042,  2204,  1998,  2023,  5791,  2001,  2036,\n        1026,  4895,  2243,  1028,  2000,  1996,  2048,  2210,  2879,\n        1005,  1055,  2008,  2209,  1996,  1026,  4895,  2243,  1028,\n        1997,  5879,  1998,  2703,  2027,  2020,  2074,  8235,  2336,\n        2024,  2411,  2187,  2041,  1997,  1996,  1026,  4895,  2243,\n        1028,  2862,  1045,  2228,  2138,  1996,  3340,  2008,  2377,\n        2068,  2035,  4961,  2039,  2024,  2107,  1037,  2502,  6337,\n        2005,  1996,  2878,  2143,  2021,  2122,  2336,  2024,  6429,\n        1998,  2323,  2022,  5868,  2005,  2054,  2027,  2031,  2589,\n        2123,  1005,  1056,  2017,  2228,  1996,  2878,  2466,  2001,\n        2061,  8403,  2138,  2009,  2001,  2995,  1998,  2001,  2619,\n        1005,  1055,  2166,  2044,  2035,  2008,  2001,  4207,  2007,\n        2149,  2035])"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCJ_qS72I60w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(seq):\n",
        "  return \" \".join( [reverse_index.get(i,\"?\") for i in seq] )\n",
        "x_data = [decode(i) for i in training_data]\n",
        "val_data = [decode(i) for i in testing_data]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGgle1BKQjds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1ee537d-4f24-4500-a246-44fbe497967d"
      },
      "source": [
        "x_data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "he radar after not <UNK> much at the box office and isn't even a cult classic the reason why transformers 2 is seen as acceptable by average movie goers is because they are used to seeing transformers 2 if film's as original and funny as this were <UNK> out as often as multi million pieces of s t the cinematic experience would be a much <UNK> place br br when <UNK> say they don't make em like they used to <UNK> didn't see welcome to <UNK> br br a fun mini masterpiece of caper comedy that refuses to compromise one of my favourites\",\n \"<START> well i'll be honest it is not exactly a sholay but you cant get a sholay every week in fact you could see distinct <UNK> of not without my daughter sally field 1991 in this movie however as most inspired movies go this one was a well inspired one well handled and well done <UNK> <UNK> as usual tends to <UNK> his <UNK> but all others are commendable specially so about <UNK> naval saw her after a long time but she hasn't lost any of her grace in fact she has performed much better that when i last saw her another one of the bollywood stars that seem to grow more beautiful as they age br br all in all a nice watch\",\n \"<START> ripoff of a dozen better films particularly steven <UNK> la story which at least had the grace to be obviously fictional even though it starred his then girlfriend playing his girlfriend in the film br br yes naive boys and girls 20 dates is a <UNK> although i am not absolutely certain that was <UNK> <UNK> intent when he started my impression is that he started the project semi seriously then quickly realized that it would be pathetic and not funny unless he made the situations more and more ridiculous as a result the whole thing has an uneasy cheap and <UNK> feeling about it br br as someone <UNK> pointed out the film has two of the dates <UNK> and putting <UNK> orders on <UNK> and yet they appear in the film which would be impossible as it would require a <UNK> form it also appears to me that the majority of women who appear as the dates are professional actresses <UNK> not famous ones <UNK> tina <UNK> they are simply too obviously pretty polished thin and comfortable in front of the camera to be average civilians br br mr <UNK> makes a classic error in only casting this kind of very pretty thin actress instead of <UNK> a variety of believable women which might have made the premise even in a <UNK> more believable and funnier he also <UNK> over what is probably his real world problem and which is that both the movie character and the real world <UNK> <UNK> appear to be <UNK> unemployed his real life imdb credits are practically non existent <UNK> this film even in the world of the movie his ex wife divorced him for never being employed i think the viewer let alone mr <UNK> real life dates are deserving of an explanation of he manages to live in one of the most expensive urban <UNK> in the us in a luxury apartment driving a fancy car and eating out at <UNK> <UNK> when he doesn't seen to have any source of <UNK> whatsoever is he drug dealer living off his rich parents no clue br br you can get away with most anything in a film if the jokes are really funny 20 dates is painfully embarrassingly un funny mr <UNK> idea of a joke is to have his character while on restaurant dates <UNK> to his companions how the food served is likely to give him either <UNK> or <UNK> the worst kind of childish <UNK> humor br br it is not very surprising to discover that mr <UNK> never made a film before 20 dates and in the last 8 years has not made a single film appeared as an actor in anyone else's film or had a writing or producing credit of any kind my gut instinct tells me that this film was not <UNK> by <UNK> the gangster money man who appears off camera but more likely by mr <UNK> <UNK> parents or perhaps represents a shocking abuse of credit cards <UNK> it was we can all rest easy that we are unlikely to have to see <UNK> <UNK> or any of his creative efforts ever again <UNK>\",\n \"<START> i found this movie really funny because you have a youthful black comedian chris rock who dies and is sent back to earth in a mid 50's white mans body he doesn't realize that his behavior should change and continues to act as he had before he <UNK> to rap music sings along and plays the stereotypical part of an urban black man the real humor in this movie was watching the trouble that this behavior gets him into with the black community\",\n \"<START> a true wholesome american story about teenagers who are interested in <UNK> their own rocket in a rural west virginia coal mining town after the launch of <UNK> in 1957 br br through trial <UNK> and <UNK> beyond belief they are ultimately able to achieve their goals br br jake <UNK> as the leader of the group is excellent in the title role as his <UNK> science teacher laura <UNK> is quite good but her southern accent is over the top br br there is a standout supporting performance by chris cooper a head <UNK> who wants his son to follow in his footsteps but gradually comes around at film's end br br what makes this film so unusual for our times is that there are no bed <UNK> scenes and no profanity whatsoever it is the epitome of an american story that is well done br br besides the science angle we have the father son <UNK> football <UNK> as a way to escape coal mining and the loving spirit of family br br why aren't pictures like this recognized more at award times\",\n \"<START> i couldn't believe it when i put this movie in my dvd player i thought i'd have a good laugh since i've played d d for half my life i had to turn it off as i had company and they were wondering what the crap i was watching br br finished it later and i should have just left it off at the soft core gay clown porn in the beginning no they run the gamut of fart jokes <UNK> jokes incest racism dressing up as <UNK> this movie is flat out mean to anyone who's ever played d d br br no wonder it looks like the real d d wouldn't let them use their game who'd want their name attached to this\",\n \"<START> men of honor features cuba gooding jr in what is probably his best performance to date he plays carl brashear a man of <UNK> courage and <UNK> he's a poor dirt farmer from the south who wants to become a navy diver but has problems because of his race the head of the diving school played by robert deniro is a racist redneck that nonetheless grows to respect brashear the film is about how brashear has to <UNK> the nearly <UNK> odds not once but twice the performances are what make this film special gooding is great and deniro the best actor in movie history gives a <UNK> performance his best dramatic work in years <UNK> <UNK> gives another solid performance as <UNK> much younger wife the film lays on the <UNK> a little too strong though no where near the level it was in the <UNK> and a few of the characters are just one <UNK> bad hal <UNK> mr <UNK> is just so evil but the film is a <UNK> among films today it's uplifting and <UNK> a wonderful film\",\n \"<START> i really think that this movie is great personally but in every movie there is a <UNK> now some of you may not have watched <UNK> <UNK> <UNK> your <UNK> but if you think about it those two shows are very very similar if you know what i mean in <UNK> new <UNK> holly wants so bad to get into <UNK> <UNK> in <UNK> your <UNK> <UNK> wants to go to a <UNK> in l a don't remember the name of the <UNK> there they are both in the music field and they both have to sing at the ending of the <UNK> it's really funny how these two films are alike i personally like <UNK> new <UNK> better than <UNK> your <UNK> though\",\n \"<START> i don't remember seeing another murder mystery movie as bad as this this movie about a medical <UNK> who <UNK> his friend's mysterious death in a car accident has the complete <UNK> for a bad movie bad acting boring story lack of suspense poor humor and no drama i remembered seeing this movie on <UNK> a tv station notable for <UNK> out low budgeted and campy made for tv movies such as this one tv movies of course do not have the edge factor or the suspense as movies from the big screen but this movie sure hit all sour tastes the makers of this movie have missed out on an opportunity to making <UNK> for murder a great tv movie the title does offer some suspense br br so if you want a good recipe don't watch this movie this movie alone can kill your tv <UNK> br br grade f\",\n \"<START> no day passes without a new released computer animated movie so we now really have chances to see more than some nice effects after watching ice age i felt that's it was not that big impact on me than some other films of this genre br br but it's because i am a big guy now and i am pretty sure that this is a very enjoyable movie for children maybe up to 14 the story is quite simple and the actors are funny in a cute way without any crude or complex humour even the evil is lovely <UNK> big cat with those funny teeth and the story has a happy end which was a small disappointment for me knowing that most of the main characters are doomed to <UNK> in a sad way but a great thing for children and apart from some fights nobody dies not even when he gets <UNK> on by a <UNK> several times which made a <UNK> feeling br br the computer animation part is nice but nothing special apart from some really nice <UNK> feeling scenes when you feel like walking in a nice painting or pages of a comics which means lots of work nevertheless br br there were some gags which made me smile i accept the creators tried to satisfy those <UNK> but they are hard to spot and in my opinion better left <UNK> since it does not feel to fit into the story br br overall it's a nice movie but it's rather in the ideal world and <UNK> animals for children disney cliche if you don't hate cute animals making funny things watch it at least once\",\n \"<START> a different look at horror the <UNK> differences between american and russian films is interesting however from my american perspective this movie just wasn't that good the protagonist marie played by <UNK> <UNK> wasn't a pleasant character and i had a hard time <UNK> with her she was <UNK> most of the time and confused for much of what little time was left also too much time was spent in bringing her to the main location of the film then a long time passed before any real suspense built up once that happened it seemed volume was used as the main effect which was more annoying than anything else the concept was more original than most direct to video movies and they didn't use sex to make up for a thin plot all in all i'd recommend it for renting but not for theater goers\",\n \"<START> the house of the spirits is quite awful i live in south america in a country that suffered a military <UNK> just like the one the movie tries to describe and even though everyone knows movies may be far far away from reality this particular movie treats viewers as both ignorant and stupid things are not so simple and linear as appears here and of course political process are much more complicated and interesting that the plot in the house if you can't show that complexity on screen is better not making a movie at all there are a lot of examples of how can politics be seriously taken in cinema without so many <UNK> in some parts i felt that carmen miranda may appear within <UNK> and palm trees when you talk about certain things you must be not only careful but <UNK> to your <UNK> intelligence\",\n \"<START> ok here is my personal list of top <UNK> shows as in today br br 1 all grown up <UNK> <UNK> br br 2 my life as a teenage robot br br 3 <UNK> <UNK> br br 4 <UNK> <UNK> br br notice a word with only capital letters that means this is the nick show i'm going to talk about br br <UNK> is basically a simple but great animated comedy about three wealthy cats mr <UNK> gordon and <UNK> who get into weird and really surreal situations from attempting to join human <UNK> <UNK> party for root beer to saving a planet of slugs from the evil spaceship this is one nick show that you will simply have your funny bone <UNK> sooner or later the theme song is catchy and memorable voice actors including wayne knight from the <UNK> franchise brings the characters to fresh life with very quirky personalities the stories are enjoyable <UNK> episodes would be king of all root beer and <UNK> lucky claw and the humor is all done in some style of <UNK> jim br br so in conclusion <UNK> is one of the <UNK> series like <UNK> <UNK> and <UNK> which becomes very very popular all over the world in just 3 seasons or less\",\n '<START> when i say doctor who you might <UNK> up an image of tom baker or jon pertwee or maybe peter <UNK> when i say james bond you ll almost certainly <UNK> up an image of sean connery while a small handful of people may think of roger moore or pierce brosnan but when i say sgt <UNK> absolutely everyone will think of phil <UNK> unlike doctor who or james bond the role belongs exclusively to one actor and that s the problem with this film version you ll continually wish you were watching the old black and white show in fact the whole idea of making a film version of <UNK> without <UNK> in the title role comes close to <UNK>',\n \"<START> one of those classics held up next to deep throat and behind the green door br br sure it was clever but the female lead isn't that attractive and sex isn't that hot but if not for this film porn would not have <UNK> into what it is today br br harry <UNK> was the ron jeremy of his day worth a look if you're a fan\",\n \"<START> this movie was the <UNK> mormon movie made yet it made the <UNK> sons of <UNK> look like well done films it was supposed to be funny from what i was told the best part was the best actor in the movie travis <UNK> if he wasn't in the movie it probably wouldn't have been made he ruled br br 10 it wasn't funny 9 it was beat 8 it had <UNK> big t <UNK> who's character made no sense 7 it was made in <UNK> 6 it didn't make fun of <UNK> 5 it had larry h miller in it 4 it was the 1st movie clint howard wasn't funny in 3 gary coleman chose the perfect movie 4 a comeback 2 they should have cast at surreal life <UNK> 1 it was made by <UNK> entertainment\",\n \"<START> i think this still is the best routine there are some others like <UNK> bring the pain and allen's men are pigs that are hilarious damon <UNK> last stand is also funny in a <UNK> way but this routine has no errors all the jokes are funny and the time limit of 70 minutes is perfect just long enough to last 20 years i just love how he allows the audience to be totally themselves and <UNK> i'm a fan of the classics and for a guy who watched a lot of of jim carrey growing up watching a more laid back comic is pretty cool not putting in a category with ellen and <UNK> but something you can watch if you're <UNK> thanks eddie god bless\",\n '<START> i was thirteen years old when i saw this movie i expected a lot of action since escape from new york was 16 rated in germany i entered the movie as <UNK> it was so boring afterwards i realized that this was just crap where a husband <UNK> his wife i mean today you do this via internet and you pay for instant access it is more then 20 years ago but i am still angry that i waste my time with this film this is a soft porno for <UNK> <UNK> bo derek and painting her with color nice but then they should named the film <UNK> bo and painting her',\n '<START> for the record i am not <UNK> with the production in any way br br hidden frontier is probably the star trek fan film with the most episodes produced to date over 7 seasons this is the last they have produced some 50 or so episodes br br this is no mean feat on almost no budget and everyone <UNK> their time and energy br br by their own admission the earlier seasons do not have as good production qualities as later ones but as they progress the effects green screen work and acting all improve br br i did find it difficult to <UNK> into so started from the beginning and watched all the way through <UNK> benefits from story <UNK> just like all the best sci fi and <UNK> nicely into the star trek universe in which it is set characters and relatives from the original series have been brought into the stories and add a lot to the feel of the stories sometimes <UNK> on the characters over the original br br the whole experience includes an excellent web site <UNK> <UNK> a high <UNK> forum which is <UNK> by many of the actors and production staff and a weekly chat br br if you are looking for high definition high budget productions this is probably not for you br br if you are looking for continued adventures in the star trek universe with stories that does star trek credit and makes you think this is the one',\n '<START> the super sexy b movie actress has another bit part as future <UNK> star ray <UNK> girlfriend in this box office bomb she plays marion has only one line of dialog well one word of dialog actually she shouts out joe as <UNK> character is <UNK> poor pia <UNK> with a plastic garden <UNK> <UNK> this movie is so bad though it becomes funny hilarious at times the guys at mystery science theater 3000 would love this check out the hysterical scene at the end where pia has a nervous breakdown and all the cheesy editing and effects they do to try and show how badly <UNK> character is freaking out pia plays an aspiring hollywood screenwriter in this pia <UNK> as a screenwriter yeah right pia can barely talk let alone write pia is utterly and absolutely miscast in this dumb role but who cares the real star is the hot and fresh glory <UNK> in her bit part in this <UNK> opinion rock on glory',\n \"<START> <UNK> has helped me with my own <UNK> and <UNK> i'm a middle aged married father of two i'm quite <UNK> in my personal and professional life still i have pain from my past that i use <UNK> to <UNK> and issues from which i am slowly recovering when these <UNK> and their families share their lives with me they help me to improve my life and my relationship with my family br br the show unlike many others <UNK> into the past of the addict and reveals events that probably caused their addiction many of us suffer because it's too scary to go back and do as alice miller says the discovery and emotional acceptance of the truth in the individual and unique history of our childhood the show deserves a lot of credit for at least getting this process started this digging is painful and difficult but worth it so much coverage of addiction fictional and non fictional seems to ignore the underlying issues often it's assumed that the addict just one day started to shoot up or whatever for fun or pleasure or self interest and now they can't stop not so <UNK> are about killing pain i can relate to the different events and <UNK> in people's lives there are common themes and surprising exceptions many <UNK> have suffered miserable abuse some kids simply respond badly to divorce to those who think that addiction is an over reaction to a <UNK> i would just say that different people respond differently although some kids handle divorce well others like <UNK> in the show collapse in a heap on the floor and have their lives forever changed by the event br br for example last <UNK> <UNK> said that pretty young <UNK> seeks <UNK> from men she <UNK> for cash for a 75 year old neighbor and lets men abuse her sound familiar to anyone the series is filled with information that we can use to understand our own motivations and make <UNK> to our lives often it's those of us with smaller issues who suffer the longest as they say even a stopped watch is right twice a day but a slow watch can go <UNK> for quite a while until it's made your life miserable br br to the producers thank you for making the show for digging into the past for the follow ups also the graphics the format and the theme music are brilliant br br to the <UNK> thank you for your courage to share whether or not you have helped yourself you have helped me\",\n ...]"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "882VsS0PGfJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = bert_encode(x_data,tokenizer,max_seq_length)\n",
        "val_data = bert_encode(val_data,tokenizer,max_seq_length)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UVh9Rv6FiVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "add807af-4902-4661-b924-71ffb6b9220f",
        "tags": []
      },
      "source": [
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "size = 5\n",
        "epochs = 5\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "model.fit(\n",
        "     x_data, training_targets,\n",
        "     validation_data=(val_data, testing_targets),\n",
        "      batch_size=size,\n",
        "      epochs=epochs)\n",
        "\n",
        "xx = model.predict(x_data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/5\n25000/25000 [==============================] - 1366s 55ms/sample - loss: 0.6969 - accuracy: 0.5000 - val_loss: 0.6949 - val_accuracy: 0.5000\nEpoch 2/5\n25000/25000 [==============================] - 1349s 54ms/sample - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6974 - val_accuracy: 0.5000\nEpoch 3/5\n25000/25000 [==============================] - 1347s 54ms/sample - loss: 0.6955 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\nEpoch 4/5\n 5660/25000 [=====>........................] - ETA: 14:27 - loss: 0.6951 - accuracy: 0.4916"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-2b80b6bd4d69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       epochs=epochs)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.conda\\envs\\dl_nlp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J1Ij2QCivB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "2a3884af-264c-493a-d8dc-7a1760315191"
      },
      "source": [
        "x_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywpFE_FE0F6Q",
        "colab_type": "text"
      },
      "source": [
        "For your homework:\n",
        "\n",
        "1.   Run the network with batch size 5,10, 15 and 100.\n",
        "2.   Repeat (1) but this time, re-run the notebook with seqience length 500\n",
        "3.   What do you notice about the loss and accuracy?\n",
        "4.   What does this tell you about BERT vs an RNN we used earlier\n",
        "\n"
      ]
    }
  ]
}