{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGupta_homework7_summary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3FNSCb/bUQH35pL6H3J/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngupta23/ds7337_nlp/blob/master/Homework/NGupta_homework7_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwWZR4GdMV1",
        "colab_type": "text"
      },
      "source": [
        "# 1. Run the network with batch size 5,10, 15 and 100.\n",
        "\n",
        "The run with a batch size of 5, 10, 15 and 100 and with a max sequence length of 50 can be found in the notebook `NGupta_homework7_max_seq_50_rmsprop.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3DiqEbLdctF",
        "colab_type": "text"
      },
      "source": [
        "# 2. Repeat (1) but this time, re-run the notebook with sequence length 500\n",
        "\n",
        "The run with a batch size of 5, 10, 15 and 100 and with a max sequence length of 500 can be found in the notebook `NGupta_homework7_max_seq_500_rmsprop.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sifruhOnduo7",
        "colab_type": "text"
      },
      "source": [
        "# 3. What do you notice about the loss and accuracy?\n",
        "\n",
        "From both the runs, the BERT model had issues with the training at the beginning. Even after 5 epocs, the validation accuracy is stuck at 0.5 which for a binary classification problem is no better than random guessing. In addition, for the longer sequence length of 500, only batch size of 5 seemed to run till completion. The remaining batch sizes ran out of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5o4J-xjd7p6",
        "colab_type": "text"
      },
      "source": [
        "# 4. What does this tell you about BERT vs an RNN we used earlier\n",
        "\n",
        "Comparing this to the same data that we modeled using RNNs ealier, we find that the RNN variants could achive an accuracy of close to 80% pretty easily and using mostly default setup parameters. In contrast, it seems that the BERT model required much more setup before things can get up and running. For example, I tried using a warm up period for the optimizer by ramping up the learning rate and then down, but this did not seem to help. The accuracy was still stuck at 0.5 after 5 epochs (see `NGupta_homework7_max_seq_50_adam_weight_decay.ipynb`). In addition, the training time for BERT models seems to be higher than what was needed for the RNNs and in many settings, the memory is not enough to complete the training. \n",
        "\n",
        "In conclusion, it seems that for simpler tasks, RNN (and its variants) might be all that is needed and that BERT might be an overkill. BERT may only be needed to be deployed when the RNNs variants do not meet the needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76o_ncK6d4vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}