{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW6: BERT Embeddings with TensorFlow 2.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe",
      "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJEKNAWFu5F",
        "colab_type": "text"
      },
      "source": [
        "This Notebook was originally written by Tensorflow and has been modified by R. D. Slater to run properly with recent changes.  Although the original worked--new changes have caused a runtime error in the .predict() function which I beleive to be due to tensor shapes (None,1,128) vs (None,128) or data type lists.  I modified the functions that produce embedding to return numpy arrays and the model now works as before.  Note you can also pass tensors (tf.convert_to_tensor()) as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IjSWx7-O8yY",
        "colab_type": "text"
      },
      "source": [
        "# BERT Embeddings with TensorFlow 2.0\n",
        "With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n",
        "- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "- See BERT on GitHub: https://github.com/google-research/bert\n",
        "- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
        "- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQktrOSAPq_n",
        "colab_type": "text"
      },
      "source": [
        "## Update TF\n",
        "We need Tensorflow 2.2 and TensorHub 0.7 for this Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iwew0KP8vRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "91cf5be2-51ad-4f52-af8a-6f63eee8826a",
        "tags": []
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: bert-for-tf2 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.14.4)\nRequirement already satisfied: py-params>=0.9.6 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.9.7)\nRequirement already satisfied: params-flow>=0.8.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.8.2)\nRequirement already satisfied: numpy in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\nRequirement already satisfied: tqdm in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.46.0)\nRequirement already satisfied: sentencepiece in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.1.91)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow_hub in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.8.0)\nRequirement already satisfied: numpy>=1.12.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from tensorflow_hub) (1.18.1)\nRequirement already satisfied: six>=1.12.0 in c:\\users\\nikhil\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow_hub) (1.12.0)\nRequirement already satisfied: protobuf>=3.8.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from tensorflow_hub) (3.11.4)\nRequirement already satisfied: setuptools in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (46.2.0.post20200511)\n"
        }
      ],
      "source": [
        "!pip install \"tensorflow_hub\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-yr9ulP_E-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "de06b156-804d-4164-fdf3-8e5afacf9738",
        "tags": []
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF version: \", tf.__version__) # 2.2\n",
        "print(\"Hub version: \", hub.__version__) # 0.8"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TF version:  2.1.0\nHub version:  0.8.0\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMeXU54uQUew",
        "colab_type": "text"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBfktvAc-CNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Tokenizer \n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU2OpvYrRFNf",
        "colab_type": "text"
      },
      "source": [
        "Building model using tf.keras and hub. from sentences to embeddings.\n",
        "\n",
        "Inputs:\n",
        " - input token ids (tokenizer converts tokens using vocab file)\n",
        " - input masks (1 for useful tokens, 0 for padding)\n",
        " - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n",
        "\n",
        "Outputs:\n",
        " - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences \n",
        " - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW6V3afD-q1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 128  # Your choice here.\n",
        "\n",
        "# Three Inputs to the BERT model\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "# Bert Layer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmR3jHYE_y3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDpzy1-STOh",
        "colab_type": "text"
      },
      "source": [
        "Generating segments and masks based on the original BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Y_r3lmFO1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "\n",
        "###############################\n",
        "# Robert Slater: Modifications to these functions to simply return numpy arrays \n",
        "###############################\n",
        "\n",
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return np.array([1]*len(tokens) + [0] * (max_seq_length - len(tokens)))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return np.array(segments + [0] * (max_seq_length - len(tokens)))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return np.array(input_ids)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44u2pruZSbMX",
        "colab_type": "text"
      },
      "source": [
        "Import tokenizer using the original vocab file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[1;31mInit signature:\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m      Runs end-to-end tokenziation.\n\u001b[1;31mFile:\u001b[0m           c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\bert\\tokenization\\bert_tokenization.py\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     \n"
        }
      ],
      "source": [
        "?FullTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm3lGfQb-1J8",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "print(vocab_file, ' | ' , do_lower_case)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "b'C:\\\\Users\\\\Nikhil\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\ce53fe6769d2ac3a260e92555120c54e1aecbea6\\\\assets\\\\vocab.txt'  |  True\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5ZV6MtqSmLa",
        "colab_type": "text"
      },
      "source": [
        "## Test BERT embedding generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVkTaR0lCcCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[2,45,15,706]  # TODO: What is this list?\n",
        "s = \"This movie is bad\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AihvrFWcSzd6",
        "colab_type": "text"
      },
      "source": [
        "Tokenizing the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X798BKV_Co71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stokens = tokenizer.tokenize(s)\n",
        "stokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "['this', 'movie', 'is', 'bad']"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxroAvbjStsX",
        "colab_type": "text"
      },
      "source": [
        "Adding separator tokens according to the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znzQLHURDAfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "stokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "['[CLS]', 'this', 'movie', 'is', 'bad', '[SEP]']"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRwyPuauTc2z",
        "colab_type": "text"
      },
      "source": [
        "Get the model inputs from the tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyyjWY75Flns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
        "input_masks = get_masks(stokens, max_seq_length)\n",
        "input_segments = get_segments(stokens, max_seq_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgKzlBloMX_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b3d0854-3c6d-46ac-ea90-92534f76b225",
        "tags": []
      },
      "source": [
        "print(f\"Tokens: {stokens}\")\n",
        "print(f\"Input IDs:\\n {input_ids}\")\n",
        "print(f\"Input Masks:\\n {input_masks}\")\n",
        "print(f\"Input Segments:\\n {input_segments}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokens: ['[CLS]', 'this', 'movie', 'is', 'bad', '[SEP]']\nInput IDs:\n [ 101 2023 3185 2003 2919  102    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\nInput Masks:\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nInput Segments:\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi2mj4EUTi0X",
        "colab_type": "text"
      },
      "source": [
        "Generate Embeddings using the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik3xqHqXM_lN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "ce5680b0-b2ae-40fc-f11c-46b76990760e",
        "tags": []
      },
      "source": [
        "# Expect a shape Wawrning.  I beleive this is due to eager execution, but not sure\n",
        "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "\n",
        "## TODO: I am getting a different warning than the one in the original notebook. "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4 batches). You may need to use the repeat() function when building your dataset.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wh7l5ybTqbh",
        "colab_type": "text"
      },
      "source": [
        "## Pooled embedding vs [CLS] as sentence-level representation\n",
        "\n",
        "Previously, the [CLS] token's embedding were used as sentence-level representation (see the original paper). However, here a pooled embedding were introduced. This part is a short comparison of the two embedding using cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5hNr1u4Nhkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def square_rooted(x):\n",
        "    return math.sqrt(sum([a*a for a in x]))\n",
        "\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return numerator/float(denominator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7ZrIs93Oc0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b92eb751-1bd7-400d-b035-277925304b28"
      },
      "source": [
        "cosine_similarity(pool_embs[0], all_embs[0][0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.030847375908003807"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JKOs0VqzYvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "731d850e-a835-49d5-f492-c15bf5b1aeb4"
      },
      "source": [
        "cosine_similarity(pool_embs[0], all_embs[0][0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.030847375908003807"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKml_uU9DnI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "bc7826c8-a6a5-429c-9346-d5ed7ea00b06",
        "tags": []
      },
      "source": [
        "model.summary()\n",
        "## TODO: Also, below, my shape is (None, 128) whereas the one in the original notebook is (None, 512)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n==================================================================================================\nTotal params: 109,482,241\nTrainable params: 109,482,240\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ddHDPuFOGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0b588eab-3ec2-4ec4-b84f-525deeae5439"
      },
      "source": [
        "pool_embs.shape\n",
        "## TODO: Also, below, my shape is (1, 768) whereas the one in the original notebook is (512, 768)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1, 768)"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ROwvSPFVxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e16555db-8856-412a-85e2-0385aca50fa1"
      },
      "source": [
        "all_embs.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1, 128, 768)"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TxL2myUHIE9",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "**Take the imdb database data set.  Convert reviews into text and then create data to be put into BERT.**\n",
        "\n",
        "## 1.   Load imdb dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab_type": "text",
        "id": "Ycq-gTVuHHBi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loaded dataset with 25000 training samples, 25000 test samples\n"
        }
      ],
      "source": [
        "vocabulary_size = 5000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "---review---\n218\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n---label---\n1\n"
        }
      ],
      "source": [
        "print('---review---')\n",
        "print(len(X_train[0]))\n",
        "print(X_train[0])\n",
        "print('---label---')\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{0, 1} | {0, 1}\n"
        }
      ],
      "source": [
        "print(set(y_train), \"|\", set(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert integers from imdb dictionary to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'dict'> | 88584\n1: the\n2: and\n3: a\n4: of\n5: to\n6: is\n7: br\n8: in\n9: it\n10: i\n11: this\n12: that\n13: was\n14: as\n"
        }
      ],
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "print(type(word_index), \"|\", len(word_index))\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "for i in range(1,15):\n",
        "    print(f\"{i}: {reverse_word_index[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "numpy.ndarray"
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for i in np.arange(len(X_train)):\n",
        "    sentence = [reverse_word_index[word_index] for word_index in X_train[i]]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "X_train_decoded = np.array(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n['the', 'thought', 'solid', 'thought', 'and', 'do', 'making', 'to', 'is', 'spot', 'nomination', 'and', 'while', 'he', 'of', 'jack', 'in', 'where', 'picked', 'as', 'getting', 'on', 'was', 'did', 'hands', 'fact', 'characters', 'to', 'always', 'life', 'thrillers', 'not', 'as', 'me', \"can't\", 'in', 'at', 'are', 'br', 'of', 'sure', 'your', 'way', 'of', 'little', 'it', 'strongly', 'random', 'to', 'view', 'of', 'love', 'it', 'so', 'and', 'of', 'guy', 'it', 'used', 'producer', 'of', 'where', 'it', 'of', 'here', 'icon', 'film', 'of', 'outside', 'to', \"don't\", 'all', 'unique', 'some', 'like', 'of', 'direction', 'it', 'if', 'out', 'her', 'imagination', 'below', 'keep', 'of', 'queen', 'he', 'and', 'to', 'makes', 'this', 'stretch', 'and', 'of', 'solid', 'it', 'thought', 'begins', 'br', 'and', 'and', 'budget', 'worthwhile', 'though', 'ok', 'and', 'and', 'for', 'ever', 'better', 'were', 'and', 'and', 'for', 'budget', 'look', 'kicked', 'any', 'to', 'of', 'making', 'it', 'out', 'and', 'follows', 'for', 'effects', 'show', 'to', 'show', 'cast', 'this', 'family', 'us', 'scenes', 'more', 'it', 'severe', 'making', 'and', 'to', 'and', 'finds', 'tv', 'tend', 'to', 'of', 'and', 'these', 'thing', 'wants', 'but', 'and', 'an', 'and', 'cult', 'as', 'it', 'is', 'video', 'do', 'you', 'david', 'see', 'scenery', 'it', 'in', 'few', 'those', 'are', 'of', 'ship', 'for', 'with', 'of', 'wild', 'to', 'one', 'is', 'very', 'work', 'dark', 'they', \"don't\", 'do', 'dvd', 'with', 'those', 'them']\n"
        }
      ],
      "source": [
        "print(X_train_decoded.shape)\n",
        "print(X_train[1])\n",
        "print(X_train_decoded[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for i in np.arange(len(X_test)):\n",
        "    sentence = [reverse_word_index[word_index] for word_index in X_test[i]]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "X_test_decoded = np.array(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n[1, 14, 22, 3443, 6, 176, 7, 2, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 2, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 2, 2, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 2, 185, 132, 1988, 2, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 2, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 2, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 3077]\n['the', 'as', 'you', \"world's\", 'is', 'quite', 'br', 'and', 'most', 'that', 'quest', 'are', 'chase', 'to', 'being', 'quickly', 'of', 'little', 'it', 'time', 'hell', 'to', 'plot', 'br', 'of', 'something', 'long', 'put', 'are', 'of', 'every', 'place', 'this', 'and', 'and', 'of', 'and', 'storytelling', 'being', 'nasty', 'not', 'of', 'you', 'warren', 'in', 'is', 'failed', 'club', 'i', 'i', 'of', 'films', 'pay', 'so', 'sequences', 'and', 'film', 'okay', 'uses', 'to', 'received', 'and', 'if', 'time', 'done', 'for', 'room', 'and', 'viewer', 'as', 'cartoon', 'of', 'gives', 'to', 'forgettable', 'br', 'be', 'because', 'many', 'these', 'of', 'and', 'and', 'contained', 'gives', 'it', 'wreck', 'scene', 'to', 'more', 'was', 'two', 'when', 'had', 'find', 'as', 'you', 'another', 'it', 'of', 'themselves', 'probably', 'who', 'and', 'storytelling', 'if', 'itself', 'by', 'br', 'about', \"1950's\", 'films', 'not', 'would', 'effects', 'that', 'her', 'box', 'to', 'miike', 'for', 'if', 'hero', 'close', 'seek', 'end', 'is', 'very', 'together', 'movie', 'of', 'and', 'got', 'say', 'kong', 'and', 'fred', 'close', 'bore', 'there', 'is', 'playing', 'lot', 'of', 'and', 'pan', 'place', 'trilogy', 'of', 'lacks', 'br', 'of', 'their', 'time', 'much', 'this', 'men', 'as', 'on', 'it', 'is', 'telling', 'program', 'br', 'and', 'okay', 'and', 'to', 'frustration', 'at', 'corner', 'and', 'she', 'of', 'sequences', 'to', 'political', 'clearly', 'in', 'of', 'drugs', 'keep', 'guy', 'i', 'i', 'was', 'throwing', 'room', 'and', 'as', 'it', 'by', 'br', 'be', 'plot', 'many', 'for', 'occasionally', 'film', 'and', 'boyfriend', 'difficult', 'kid', 'as', 'you', 'it', 'failed', 'not', 'if', 'gerard', 'to', 'if', 'woman', 'in', 'and', 'is', 'police', 'fi', 'spooky', 'or', 'of', 'self', 'what', 'have', 'pretty', 'in', 'can', 'so', 'suit', 'you', 'good', '2', 'which', 'why', 'super', 'as', 'it', 'main', 'of', 'my', 'i', 'i', '\\x96', 'if', 'time', 'screenplay', 'in', 'same', 'this', 'remember', 'and', 'have', 'action', 'one', 'in', 'realistic', 'that', 'better', 'of', 'lessons']\n"
        }
      ],
      "source": [
        "print(X_test_decoded.shape)\n",
        "print(X_test[1])\n",
        "print(X_test_decoded[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenize and convert the text to integers for BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence: List, tokenizer, max_seq_length: int) -> List:\n",
        "    \"\"\"\n",
        "    Tokenize a single sentence for BERT\n",
        "      1. Tokenizes the sentence\n",
        "      2. Chops off excess words\n",
        "      3. Adds the CLS and SEP tokens\n",
        "\n",
        "    :param sentence A \"single\" sentence that needs to be tokenized.\n",
        "    :type sentence List\n",
        "    :param tokenizer The BERT tokenizer\n",
        "    :type tokenizer <TBD>\n",
        "    :param max_seq_length The maximum sequence length to use (including the CLS and SEP tokens)\n",
        "    :type max_seq_length int\n",
        "    :rtype List\n",
        "    \"\"\"\n",
        "    # Tokenize Sentence\n",
        "    stokens = tokenizer.tokenize(\" \".join(sentence))\n",
        "\n",
        "    # Chop off excess\n",
        "    if len(stokens) > (max_seq_length - 2):\n",
        "        stokens = stokens[:(max_seq_length - 2)]\n",
        "\n",
        "    # Add [CLS] and [SEP] tokens\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "    return stokens\n",
        "\n",
        "def tokenize_all_data(data: np.ndarray, tokenizer, max_seq_length: int):\n",
        "    \"\"\"\n",
        "    Takes the complete data (multiple sentence) and tokenizes it for BERT\n",
        "    For each sentence (row of data), performs the following steps\n",
        "      1. Tokenizes the sentence\n",
        "      2. Chops off excess words\n",
        "      3. Adds the CLS and SEP tokens\n",
        "\n",
        "    :param data A complete data (comprising of multiple sentences) that needs to be tokenized.\n",
        "    :type data np.ndarray\n",
        "    :param tokenizer The BERT tokenizer\n",
        "    :type tokenizer <TBD>\n",
        "    :param max_seq_length The maximum sequence length to use (including the CLS and SEP tokens)\n",
        "    :type max_seq_length int\n",
        "    :rtype List\n",
        "    \"\"\"\n",
        "    data_tokens = []\n",
        "    for i in np.arange(len(data)):\n",
        "        stokens = tokenize_sentence(sentence=data[i], tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "        data_tokens.append(stokens)\n",
        "\n",
        "    data_tokens = np.array(data_tokens)\n",
        "    return data_tokens\n",
        "\n",
        "def get_ids_from_tokenized_data(data_tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to IDs for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_ids = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_ids = get_ids(data_tokens[i], tokenizer, max_seq_length)\n",
        "        data_input_ids.append(input_ids)\n",
        "\n",
        "    data_input_ids = np.array(data_input_ids)\n",
        "    return data_input_ids\n",
        "\n",
        "def get_masks_from_tokenized_data(data_tokens, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to masks for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_masks = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_masks = get_masks(data_tokens[i], max_seq_length)\n",
        "        data_input_masks.append(input_masks)\n",
        "\n",
        "    data_input_masks = np.array(data_input_masks)\n",
        "    return data_input_masks\n",
        "\n",
        "def get_segments_from_tokenized_data(data_tokens, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to segments for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_segments = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_segments = get_segments(data_tokens[i], max_seq_length)\n",
        "        data_input_segments.append(input_segments)\n",
        "\n",
        "    data_input_segments = np.array(data_input_segments)\n",
        "    return data_input_segments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tokens = tokenize_all_data(data=X_train_decoded, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n----------------------------------------------------------------------------------------------------\n128 | ['[CLS]', 'the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', 'isn', \"'\", 't', '[SEP]']\n----------------------------------------------------------------------------------------------------\n45 | ['[CLS]', 'the', 'effort', 'still', 'been', 'that', 'usually', 'makes', 'for', 'of', 'finished', 'and', 'ended', 'and', 'an', 'because', 'before', 'if', 'just', 'though', 'something', 'know', 'novel', 'female', 'i', 'i', 'slowly', 'lot', 'of', 'above', 'and', 'with', 'connect', 'in', 'of', 'script', 'their', 'that', 'out', 'end', 'his', 'and', 'i', 'i', '[SEP]']\n"
        }
      ],
      "source": [
        "print(X_train_tokens.shape)\n",
        "print(\"-\"*100)\n",
        "print(len(X_train_tokens[0]), \"|\", X_train_tokens[0])\n",
        "print(\"-\"*100)\n",
        "print(len(X_train_tokens[5]), \"|\", X_train_tokens[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(25000, 128)"
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "X_train_input_ids = get_ids_from_tokenized_data(data_tokens=X_train_tokens, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "X_train_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[  101  1996  2004  2017  2007  2041  3209  3928 11082  7459  2037  4150\n  4285  2018  4988  1997  2843  2013  3087  2000  2031  2044  2041  7224\n  2196  2062  2282  1998  2009  2061  2540  3065  2000  2086  1997  2296\n  2196  2183  1998  2393  5312  2030  1997  2296  3108  5107  3185  3272\n  2014  2001  2195  1997  2438  2062  2007  2003  2085  2783  2143  2004\n  2017  1997  3067  9280  6854  1997  2017  2084  2032  2008  2007  2041\n  3209  2014  2131  2005  2001  3409  1997  2017  3185  2823  3185  2008\n  2007 12459  2021  1998  2000  2466  6919  2008  1999  3773  1999  2839\n  2000  1997 17549  1998  2007  2540  2018  6281  2027  1997  2182  2008\n  2007  2014  3809  2000  2031  2515  2043  2013  2339  2054  2031  4401\n  2027  2003  2017  2008  3475  1005  1056   102]\n----------------------------------------------------------------------------------------------------\n[ 101 1996 3947 2145 2042 2008 2788 3084 2005 1997 2736 1998 3092 1998\n 2019 2138 2077 2065 2074 2295 2242 2113 3117 2931 1045 1045 3254 2843\n 1997 2682 1998 2007 7532 1999 1997 5896 2037 2008 2041 2203 2010 1998\n 1045 1045  102    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n"
        }
      ],
      "source": [
        "print(X_train_input_ids[0])\n",
        "print(\"-\"*100)\n",
        "print(X_train_input_ids[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Repeat for Test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.  Create text Masks for BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_masks = get_masks_from_tokenized_data(data_tokens=X_train_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "128  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n----------------------------------------------------------------------------------------------------\n45  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_train_masks[0] == 1), \" | \", X_train_masks[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_train_masks[5] == 1), \" | \", X_train_masks[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Repeat for Test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create text Segments for BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_input_segments = get_segments_from_tokenized_data(data_tokens=X_train_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "128  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n----------------------------------------------------------------------------------------------------\n128  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_train_input_segments[0] == 0), \" | \", X_train_input_segments[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_train_input_segments[5] == 0), \" | \", X_train_input_segments[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Repeat for Test dataset"
      ]
    }
  ]
}