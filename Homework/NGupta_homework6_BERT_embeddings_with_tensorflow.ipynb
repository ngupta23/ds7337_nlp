{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW6: BERT Embeddings with TensorFlow 2.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe",
      "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJEKNAWFu5F",
        "colab_type": "text"
      },
      "source": [
        "This Notebook was originally written by Tensorflow and has been modified by R. D. Slater to run properly with recent changes.  Although the original worked--new changes have caused a runtime error in the .predict() function which I beleive to be due to tensor shapes (None,1,128) vs (None,128) or data type lists.  I modified the functions that produce embedding to return numpy arrays and the model now works as before.  Note you can also pass tensors (tf.convert_to_tensor()) as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IjSWx7-O8yY",
        "colab_type": "text"
      },
      "source": [
        "# BERT Embeddings with TensorFlow 2.0\n",
        "With the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n",
        "- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "- See BERT on GitHub: https://github.com/google-research/bert\n",
        "- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n",
        "- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQktrOSAPq_n",
        "colab_type": "text"
      },
      "source": [
        "## Update TF\n",
        "We need Tensorflow 2.2 and TensorHub 0.7 for this Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iwew0KP8vRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "91cf5be2-51ad-4f52-af8a-6f63eee8826a",
        "tags": []
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: bert-for-tf2 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.14.4)\nRequirement already satisfied: params-flow>=0.8.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.8.2)\nRequirement already satisfied: py-params>=0.9.6 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from bert-for-tf2) (0.9.7)\nRequirement already satisfied: numpy in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\nRequirement already satisfied: tqdm in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.46.0)\nRequirement already satisfied: sentencepiece in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.1.91)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow_hub in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (0.8.0)\nRequirement already satisfied: numpy>=1.12.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from tensorflow_hub) (1.18.1)\nRequirement already satisfied: protobuf>=3.8.0 in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from tensorflow_hub) (3.11.4)\nRequirement already satisfied: six>=1.12.0 in c:\\users\\nikhil\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow_hub) (1.12.0)\nRequirement already satisfied: setuptools in c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (46.2.0.post20200511)\n"
        }
      ],
      "source": [
        "!pip install \"tensorflow_hub\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-yr9ulP_E-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "de06b156-804d-4164-fdf3-8e5afacf9738",
        "tags": []
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF version: \", tf.__version__) # 2.2\n",
        "print(\"Hub version: \", hub.__version__) # 0.8"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TF version:  2.1.0\nHub version:  0.8.0\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMeXU54uQUew",
        "colab_type": "text"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBfktvAc-CNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Tokenizer \n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU2OpvYrRFNf",
        "colab_type": "text"
      },
      "source": [
        "Building model using tf.keras and hub. from sentences to embeddings.\n",
        "\n",
        "Inputs:\n",
        " - input token ids (tokenizer converts tokens using vocab file)\n",
        " - input masks (1 for useful tokens, 0 for padding)\n",
        " - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n",
        "\n",
        "Outputs:\n",
        " - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences \n",
        " - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW6V3afD-q1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 512  # Your choice here.\n",
        "\n",
        "# Three Inputs to the BERT model\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "# Bert Layer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmR3jHYE_y3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDpzy1-STOh",
        "colab_type": "text"
      },
      "source": [
        "Generating segments and masks based on the original BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Y_r3lmFO1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "\n",
        "###############################\n",
        "# Robert Slater: Modifications to these functions to simply return numpy arrays \n",
        "###############################\n",
        "\n",
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return np.array([1]*len(tokens) + [0] * (max_seq_length - len(tokens)))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return np.array(segments + [0] * (max_seq_length - len(tokens)))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return np.array(input_ids)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44u2pruZSbMX",
        "colab_type": "text"
      },
      "source": [
        "Import tokenizer using the original vocab file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[1;31mInit signature:\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m      Runs end-to-end tokenziation.\n\u001b[1;31mFile:\u001b[0m           c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\bert\\tokenization\\bert_tokenization.py\n\u001b[1;31mType:\u001b[0m           type\n\u001b[1;31mSubclasses:\u001b[0m     \n"
        }
      ],
      "source": [
        "?FullTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm3lGfQb-1J8",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "print(vocab_file, ' | ' , do_lower_case)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "b'C:\\\\Users\\\\Nikhil\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\ce53fe6769d2ac3a260e92555120c54e1aecbea6\\\\assets\\\\vocab.txt'  |  True\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5ZV6MtqSmLa",
        "colab_type": "text"
      },
      "source": [
        "## Test BERT embedding generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVkTaR0lCcCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = \"This movie is bad\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AihvrFWcSzd6",
        "colab_type": "text"
      },
      "source": [
        "Tokenizing the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X798BKV_Co71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stokens = tokenizer.tokenize(s)\n",
        "stokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "['this', 'movie', 'is', 'bad']"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxroAvbjStsX",
        "colab_type": "text"
      },
      "source": [
        "Adding separator tokens according to the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znzQLHURDAfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "stokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "['[CLS]', 'this', 'movie', 'is', 'bad', '[SEP]']"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRwyPuauTc2z",
        "colab_type": "text"
      },
      "source": [
        "Get the model inputs from the tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyyjWY75Flns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
        "input_masks = get_masks(stokens, max_seq_length)\n",
        "input_segments = get_segments(stokens, max_seq_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgKzlBloMX_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b3d0854-3c6d-46ac-ea90-92534f76b225",
        "tags": []
      },
      "source": [
        "print(f\"Tokens: {stokens}\")\n",
        "print(f\"Input IDs:\\n {input_ids}\")\n",
        "print(f\"Input Masks:\\n {input_masks}\")\n",
        "print(f\"Input Segments:\\n {input_segments}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokens: ['[CLS]', 'this', 'movie', 'is', 'bad', '[SEP]']\nInput IDs:\n [ 101 2023 3185 2003 2919  102    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0]\nInput Masks:\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nInput Segments:\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi2mj4EUTi0X",
        "colab_type": "text"
      },
      "source": [
        "Generate Embeddings using the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik3xqHqXM_lN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "ce5680b0-b2ae-40fc-f11c-46b76990760e",
        "tags": []
      },
      "source": [
        "# Expect a shape Wawrning.  I beleive this is due to eager execution, but not sure\n",
        "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "\n",
        "## TODO: I am getting a different warning than the one in the original notebook. "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wh7l5ybTqbh",
        "colab_type": "text"
      },
      "source": [
        "## Pooled embedding vs [CLS] as sentence-level representation\n",
        "\n",
        "Previously, the [CLS] token's embedding were used as sentence-level representation (see the original paper). However, here a pooled embedding were introduced. This part is a short comparison of the two embedding using cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5hNr1u4Nhkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def square_rooted(x):\n",
        "    return math.sqrt(sum([a*a for a in x]))\n",
        "\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return numerator/float(denominator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7ZrIs93Oc0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b92eb751-1bd7-400d-b035-277925304b28"
      },
      "source": [
        "cosine_similarity(pool_embs[0], all_embs[0][0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.03084744318764073"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JKOs0VqzYvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "731d850e-a835-49d5-f492-c15bf5b1aeb4"
      },
      "source": [
        "cosine_similarity(pool_embs[0], all_embs[0][0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.03084744318764073"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKml_uU9DnI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "bc7826c8-a6a5-429c-9346-d5ed7ea00b06",
        "tags": []
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 512)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 512)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 512)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n==================================================================================================\nTotal params: 109,482,241\nTrainable params: 109,482,240\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ddHDPuFOGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0b588eab-3ec2-4ec4-b84f-525deeae5439"
      },
      "source": [
        "pool_embs.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1, 768)"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ROwvSPFVxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e16555db-8856-412a-85e2-0385aca50fa1"
      },
      "source": [
        "all_embs.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1, 512, 768)"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TxL2myUHIE9",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "**Take the imdb database data set.  Convert reviews into text and then create data to be put into BERT.**\n",
        "\n",
        "## 1.   Load imdb dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab_type": "text",
        "id": "Ycq-gTVuHHBi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loaded dataset with 25000 training samples, 25000 test samples\n"
        }
      ],
      "source": [
        "vocabulary_size = 5000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "---review---\n218\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n---label---\n1\n"
        }
      ],
      "source": [
        "print('---review---')\n",
        "print(len(X_train[0]))\n",
        "print(X_train[0])\n",
        "print('---label---')\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{0, 1} | {0, 1}\n"
        }
      ],
      "source": [
        "print(set(y_train), \"|\", set(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert integers from imdb dictionary to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'dict'> | 88588\n1: <START>\n2: <UNK>\n3: <UNUSED>\n4: the\n5: and\n6: a\n7: of\n8: to\n9: is\n10: br\n11: in\n12: it\n13: i\n14: this\n"
        }
      ],
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "print(type(word_index), \"|\", len(word_index))\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "for i in range(1,15):\n",
        "    print(f\"{i}: {reverse_word_index[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for i in np.arange(len(X_train)):\n",
        "    sentence = [reverse_word_index[word_index] for word_index in X_train[i]]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "X_train_decoded = np.array(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n['<START>', 'big', 'hair', 'big', '<UNK>', 'bad', 'music', 'and', 'a', 'giant', 'safety', '<UNK>', 'these', 'are', 'the', 'words', 'to', 'best', 'describe', 'this', 'terrible', 'movie', 'i', 'love', 'cheesy', 'horror', 'movies', 'and', \"i've\", 'seen', 'hundreds', 'but', 'this', 'had', 'got', 'to', 'be', 'on', 'of', 'the', 'worst', 'ever', 'made', 'the', 'plot', 'is', 'paper', 'thin', 'and', 'ridiculous', 'the', 'acting', 'is', 'an', '<UNK>', 'the', 'script', 'is', 'completely', 'laughable', 'the', 'best', 'is', 'the', 'end', 'showdown', 'with', 'the', 'cop', 'and', 'how', 'he', 'worked', 'out', 'who', 'the', 'killer', 'is', \"it's\", 'just', 'so', 'damn', 'terribly', 'written', 'the', 'clothes', 'are', '<UNK>', 'and', 'funny', 'in', 'equal', '<UNK>', 'the', 'hair', 'is', 'big', 'lots', 'of', '<UNK>', '<UNK>', 'men', 'wear', 'those', 'cut', '<UNK>', '<UNK>', 'that', 'show', 'off', 'their', '<UNK>', '<UNK>', 'that', 'men', 'actually', 'wore', 'them', 'and', 'the', 'music', 'is', 'just', '<UNK>', 'trash', 'that', 'plays', 'over', 'and', 'over', 'again', 'in', 'almost', 'every', 'scene', 'there', 'is', 'trashy', 'music', '<UNK>', 'and', '<UNK>', 'taking', 'away', 'bodies', 'and', 'the', '<UNK>', 'still', \"doesn't\", 'close', 'for', '<UNK>', 'all', '<UNK>', 'aside', 'this', 'is', 'a', 'truly', 'bad', 'film', 'whose', 'only', 'charm', 'is', 'to', 'look', 'back', 'on', 'the', 'disaster', 'that', 'was', 'the', \"80's\", 'and', 'have', 'a', 'good', 'old', 'laugh', 'at', 'how', 'bad', 'everything', 'was', 'back', 'then']\n"
        }
      ],
      "source": [
        "print(X_train_decoded.shape)\n",
        "print(X_train[1])\n",
        "print(X_train_decoded[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for i in np.arange(len(X_test)):\n",
        "    sentence = [reverse_word_index[word_index] for word_index in X_test[i]]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "X_test_decoded = np.array(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n[1, 14, 22, 3443, 6, 176, 7, 2, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 2, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 2, 2, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 2, 185, 132, 1988, 2, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 2, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 2, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 3077]\n['<START>', 'this', 'film', 'requires', 'a', 'lot', 'of', '<UNK>', 'because', 'it', 'focuses', 'on', 'mood', 'and', 'character', 'development', 'the', 'plot', 'is', 'very', 'simple', 'and', 'many', 'of', 'the', 'scenes', 'take', 'place', 'on', 'the', 'same', 'set', 'in', '<UNK>', '<UNK>', 'the', '<UNK>', 'dennis', 'character', 'apartment', 'but', 'the', 'film', 'builds', 'to', 'a', 'disturbing', 'climax', 'br', 'br', 'the', 'characters', 'create', 'an', 'atmosphere', '<UNK>', 'with', 'sexual', 'tension', 'and', 'psychological', '<UNK>', \"it's\", 'very', 'interesting', 'that', 'robert', '<UNK>', 'directed', 'this', 'considering', 'the', 'style', 'and', 'structure', 'of', 'his', 'other', 'films', 'still', 'the', '<UNK>', '<UNK>', 'audio', 'style', 'is', 'evident', 'here', 'and', 'there', 'i', 'think', 'what', 'really', 'makes', 'this', 'film', 'work', 'is', 'the', 'brilliant', 'performance', 'by', '<UNK>', 'dennis', \"it's\", 'definitely', 'one', 'of', 'her', 'darker', 'characters', 'but', 'she', 'plays', 'it', 'so', 'perfectly', 'and', 'convincingly', 'that', \"it's\", 'scary', 'michael', 'burns', 'does', 'a', 'good', 'job', 'as', 'the', '<UNK>', 'young', 'man', 'regular', '<UNK>', 'player', 'michael', 'murphy', 'has', 'a', 'small', 'part', 'the', '<UNK>', 'moody', 'set', 'fits', 'the', 'content', 'of', 'the', 'story', 'very', 'well', 'in', 'short', 'this', 'movie', 'is', 'a', 'powerful', 'study', 'of', '<UNK>', 'sexual', '<UNK>', 'and', 'desperation', 'be', 'patient', '<UNK>', 'up', 'the', 'atmosphere', 'and', 'pay', 'attention', 'to', 'the', 'wonderfully', 'written', 'script', 'br', 'br', 'i', 'praise', 'robert', '<UNK>', 'this', 'is', 'one', 'of', 'his', 'many', 'films', 'that', 'deals', 'with', '<UNK>', 'fascinating', 'subject', 'matter', 'this', 'film', 'is', 'disturbing', 'but', \"it's\", 'sincere', 'and', \"it's\", 'sure', 'to', '<UNK>', 'a', 'strong', 'emotional', 'response', 'from', 'the', 'viewer', 'if', 'you', 'want', 'to', 'see', 'an', 'unusual', 'film', 'some', 'might', 'even', 'say', 'bizarre', 'this', 'is', 'worth', 'the', 'time', 'br', 'br', 'unfortunately', \"it's\", 'very', 'difficult', 'to', 'find', 'in', 'video', '<UNK>', 'you', 'may', 'have', 'to', 'buy', 'it', 'off', 'the', 'internet']\n"
        }
      ],
      "source": [
        "print(X_test_decoded.shape)\n",
        "print(X_test[1])\n",
        "print(X_test_decoded[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenize and convert the text to integers for BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def BERT_tokenize_sentence(sentence: List, tokenizer, max_seq_length: int) -> List:\n",
        "    \"\"\"\n",
        "    Tokenize a single sentence for BERT\n",
        "      1. Tokenizes the sentence\n",
        "      2. Chops off excess words\n",
        "      3. Adds the CLS and SEP tokens\n",
        "\n",
        "    :param sentence A \"single\" sentence that needs to be tokenized.\n",
        "    :type sentence List\n",
        "    :param tokenizer The BERT tokenizer\n",
        "    :type tokenizer <TBD>\n",
        "    :param max_seq_length The maximum sequence length to use (including the CLS and SEP tokens)\n",
        "    :type max_seq_length int\n",
        "    :rtype List\n",
        "    \"\"\"\n",
        "    # Tokenize Sentence\n",
        "    stokens = tokenizer.tokenize(\" \".join(sentence))\n",
        "\n",
        "    # Chop off excess\n",
        "    if len(stokens) > (max_seq_length - 2):\n",
        "        stokens = stokens[:(max_seq_length - 2)]\n",
        "\n",
        "    # Add [CLS] and [SEP] tokens\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "    return stokens\n",
        "\n",
        "def BERT_tokenize_all_data(data: np.ndarray, tokenizer, max_seq_length: int):\n",
        "    \"\"\"\n",
        "    Takes the complete data (multiple sentence) and tokenizes it for BERT\n",
        "    For each sentence (row of data), performs the following steps\n",
        "      1. Tokenizes the sentence\n",
        "      2. Chops off excess words\n",
        "      3. Adds the CLS and SEP tokens\n",
        "\n",
        "    :param data A complete data (comprising of multiple sentences) that needs to be tokenized.\n",
        "    :type data np.ndarray\n",
        "    :param tokenizer The BERT tokenizer\n",
        "    :type tokenizer <TBD>\n",
        "    :param max_seq_length The maximum sequence length to use (including the CLS and SEP tokens)\n",
        "    :type max_seq_length int\n",
        "    :rtype List\n",
        "    \"\"\"\n",
        "    data_tokens = []\n",
        "    for i in np.arange(len(data)):\n",
        "        stokens = BERT_tokenize_sentence(sentence=data[i], tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "        data_tokens.append(stokens)\n",
        "\n",
        "    data_tokens = np.array(data_tokens)\n",
        "    return data_tokens\n",
        "\n",
        "def BERT_get_ids_from_tokenized_data(data_tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to IDs for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_ids = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_ids = get_ids(data_tokens[i], tokenizer, max_seq_length)\n",
        "        data_input_ids.append(input_ids)\n",
        "\n",
        "    data_input_ids = np.array(data_input_ids)\n",
        "    return data_input_ids\n",
        "\n",
        "def BERT_get_masks_from_tokenized_data(data_tokens, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to masks for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_masks = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_masks = get_masks(data_tokens[i], max_seq_length)\n",
        "        data_input_masks.append(input_masks)\n",
        "\n",
        "    data_input_masks = np.array(data_input_masks)\n",
        "    return data_input_masks\n",
        "\n",
        "def BERT_get_segments_from_tokenized_data(data_tokens, max_seq_length):\n",
        "    \"\"\"\n",
        "    Converts the tokens to segments for BERT input\n",
        "    TODO: Complete Docstring\n",
        "    \"\"\"\n",
        "    data_input_segments = []\n",
        "    for i in np.arange(len(data_tokens)):\n",
        "        input_segments = get_segments(data_tokens[i], max_seq_length)\n",
        "        data_input_segments.append(input_segments)\n",
        "\n",
        "    data_input_segments = np.array(data_input_segments)\n",
        "    return data_input_segments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tokens = BERT_tokenize_all_data(data=X_train_decoded, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n----------------------------------------------------------------------------------------------------\n261 | ['[CLS]', '<', 'start', '>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', 'everyone', \"'\", 's', 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '<', 'un', '##k', '>', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '<', 'un', '##k', '>', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '<', 'un', '##k', '>', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', '<', 'un', '##k', '>', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '<', 'un', '##k', '>', 'to', 'the', 'two', 'little', '<', 'un', '##k', '>', 'that', 'played', 'the', '<', 'un', '##k', '>', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '<', 'un', '##k', '>', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', '<', 'un', '##k', '>', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', '<', 'un', '##k', '>', 'for', 'what', 'they', 'have', 'done', 'don', \"'\", 't', 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', 'someone', \"'\", 's', 'life', 'after', 'all', 'that', 'was', '<', 'un', '##k', '>', 'with', 'us', 'all', '[SEP]']\n----------------------------------------------------------------------------------------------------\n62 | ['[CLS]', '<', 'start', '>', 'begins', 'better', 'than', 'it', 'ends', 'funny', 'that', 'the', 'russian', '<', 'un', '##k', '>', 'crew', '<', 'un', '##k', '>', 'all', 'other', 'actors', 'it', \"'\", 's', 'like', 'those', 'scenes', 'where', 'documentary', 'shots', 'br', 'br', 'spoil', '##er', 'part', 'the', 'message', '<', 'un', '##k', '>', 'was', 'contrary', 'to', 'the', 'whole', 'story', 'it', 'just', 'does', 'not', '<', 'un', '##k', '>', 'br', 'br', '[SEP]']\n"
        }
      ],
      "source": [
        "print(X_train_tokens.shape)\n",
        "print(\"-\"*100)\n",
        "print(len(X_train_tokens[0]), \"|\", X_train_tokens[0])\n",
        "print(\"-\"*100)\n",
        "print(len(X_train_tokens[5]), \"|\", X_train_tokens[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(25000, 512)"
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_train_input_ids = BERT_get_ids_from_tokenized_data(data_tokens=X_train_tokens, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "X_train_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[  101  1026  2707  1028  2023  2143  2001  2074  8235  9179  3295 17363\n  2466  3257  3071  1005  1055  2428 10897  1996  2112  2027  2209  1998\n  2017  2071  2074  5674  2108  2045  2728  1026  4895  2243  1028  2003\n  2019  6429  3364  1998  2085  1996  2168  2108  2472  1026  4895  2243\n  1028  2269  2234  2013  1996  2168  4104  2479  2004  2870  2061  1045\n  3866  1996  2755  2045  2001  1037  2613  4434  2007  2023  2143  1996\n 25591 12629  2802  1996  2143  2020  2307  2009  2001  2074  8235  2061\n  2172  2008  1045  4149  1996  2143  2004  2574  2004  2009  2001  2207\n  2005  1026  4895  2243  1028  1998  2052 16755  2009  2000  3071  2000\n  3422  1998  1996  4875  1026  4895  2243  1028  2001  6429  2428  6639\n  2012  1996  2203  2009  2001  2061  6517  1998  2017  2113  2054  2027\n  2360  2065  2017  5390  2012  1037  2143  2009  2442  2031  2042  2204\n  1998  2023  5791  2001  2036  1026  4895  2243  1028  2000  1996  2048\n  2210  1026  4895  2243  1028  2008  2209  1996  1026  4895  2243  1028\n  1997  5879  1998  2703  2027  2020  2074  8235  2336  2024  2411  2187\n  2041  1997  1996  1026  4895  2243  1028  2862  1045  2228  2138  1996\n  3340  2008  2377  2068  2035  4961  2039  2024  2107  1037  2502  1026\n  4895  2243  1028  2005  1996  2878  2143  2021  2122  2336  2024  6429\n  1998  2323  2022  1026  4895  2243  1028  2005  2054  2027  2031  2589\n  2123  1005  1056  2017  2228  1996  2878  2466  2001  2061  8403  2138\n  2009  2001  2995  1998  2001  2619  1005  1055  2166  2044  2035  2008\n  2001  1026  4895  2243  1028  2007  2149  2035   102     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\n----------------------------------------------------------------------------------------------------\n[  101  1026  2707  1028  4269  2488  2084  2009  4515  6057  2008  1996\n  2845  1026  4895  2243  1028  3626  1026  4895  2243  1028  2035  2060\n  5889  2009  1005  1055  2066  2216  5019  2073  4516  7171  7987  7987\n 27594  2121  2112  1996  4471  1026  4895  2243  1028  2001 10043  2000\n  1996  2878  2466  2009  2074  2515  2025  1026  4895  2243  1028  7987\n  7987   102     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\n"
        }
      ],
      "source": [
        "print(X_train_input_ids[0])\n",
        "print(\"-\"*100)\n",
        "print(X_train_input_ids[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_tokens = BERT_tokenize_all_data(data=X_test_decoded, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(25000,)\n----------------------------------------------------------------------------------------------------\n97 | ['[CLS]', '<', 'start', '>', 'please', 'give', 'this', 'one', 'a', 'miss', 'br', 'br', '<', 'un', '##k', '>', '<', 'un', '##k', '>', 'and', 'the', 'rest', 'of', 'the', 'cast', '<', 'un', '##k', '>', 'terrible', 'performances', 'the', 'show', 'is', 'flat', 'flat', 'flat', 'br', 'br', 'i', 'don', \"'\", 't', 'know', 'how', 'michael', '<', 'un', '##k', '>', 'could', 'have', 'allowed', 'this', 'one', 'on', 'his', '<', 'un', '##k', '>', 'he', 'almost', 'seemed', 'to', 'know', 'this', 'wasn', \"'\", 't', 'going', 'to', 'work', 'out', 'and', 'his', 'performance', 'was', 'quite', '<', 'un', '##k', '>', 'so', 'all', 'you', '<', 'un', '##k', '>', 'fans', 'give', 'this', 'a', 'miss', '[SEP]']\n----------------------------------------------------------------------------------------------------\n171 | ['[CLS]', '<', 'start', '>', 'i', \"'\", 'm', 'absolutely', '<', 'un', '##k', '>', 'this', 'movie', 'isn', \"'\", 't', 'being', 'sold', 'all', 'who', 'love', 'this', 'movie', 'should', '<', 'un', '##k', '>', 'disney', 'and', '<', 'un', '##k', '>', 'the', 'demand', 'for', 'it', 'they', \"'\", 'd', 'eventually', 'have', 'to', 'sell', 'it', 'then', 'i', \"'\", 'd', 'buy', 'copies', 'for', 'everybody', 'i', 'know', 'everything', 'and', 'everybody', 'in', 'this', 'movie', 'did', 'a', 'good', 'job', 'and', 'i', 'haven', \"'\", 't', 'figured', 'out', 'why', 'disney', 'hasn', \"'\", 't', 'put', 'this', 'movie', 'on', 'dvd', 'or', 'on', 'vhs', 'in', 'rental', '<', 'un', '##k', '>', 'at', 'least', 'i', 'haven', \"'\", 't', 'seen', 'any', 'copies', 'this', 'is', 'a', 'wicked', 'good', 'movie', 'and', 'should', 'be', 'seen', 'by', 'all', 'the', 'kids', 'in', 'the', 'new', 'generation', 'don', \"'\", 't', 'get', 'to', 'see', 'it', 'and', 'i', 'think', 'they', 'should', 'it', 'should', 'at', 'least', 'be', 'put', 'back', 'on', 'the', 'channel', 'this', 'movie', 'doesn', \"'\", 't', 'deserve', 'a', 'cheap', '<', 'un', '##k', '>', 'it', 'deserves', 'the', 'real', 'thing', 'i', \"'\", 'm', 'them', 'now', 'this', 'movie', 'will', 'be', 'on', 'dvd', '[SEP]']\n"
        }
      ],
      "source": [
        "print(X_test_tokens.shape)\n",
        "print(\"-\"*100)\n",
        "print(len(X_test_tokens[0]), \"|\", X_test_tokens[0])\n",
        "print(\"-\"*100)\n",
        "print(len(X_test_tokens[5]), \"|\", X_test_tokens[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(25000, 512)"
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "X_test_input_ids = BERT_get_ids_from_tokenized_data(data_tokens=X_test_tokens, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "X_test_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[ 101 1026 2707 1028 3531 2507 2023 2028 1037 3335 7987 7987 1026 4895\n 2243 1028 1026 4895 2243 1028 1998 1996 2717 1997 1996 3459 1026 4895\n 2243 1028 6659 4616 1996 2265 2003 4257 4257 4257 7987 7987 1045 2123\n 1005 1056 2113 2129 2745 1026 4895 2243 1028 2071 2031 3039 2023 2028\n 2006 2010 1026 4895 2243 1028 2002 2471 2790 2000 2113 2023 2347 1005\n 1056 2183 2000 2147 2041 1998 2010 2836 2001 3243 1026 4895 2243 1028\n 2061 2035 2017 1026 4895 2243 1028 4599 2507 2023 1037 3335  102    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0]\n----------------------------------------------------------------------------------------------------\n[  101  1026  2707  1028  1045  1005  1049  7078  1026  4895  2243  1028\n  2023  3185  3475  1005  1056  2108  2853  2035  2040  2293  2023  3185\n  2323  1026  4895  2243  1028  6373  1998  1026  4895  2243  1028  1996\n  5157  2005  2009  2027  1005  1040  2776  2031  2000  5271  2009  2059\n  1045  1005  1040  4965  4809  2005  7955  1045  2113  2673  1998  7955\n  1999  2023  3185  2106  1037  2204  3105  1998  1045  4033  1005  1056\n  6618  2041  2339  6373  8440  1005  1056  2404  2023  3185  2006  4966\n  2030  2006 17550  1999 12635  1026  4895  2243  1028  2012  2560  1045\n  4033  1005  1056  2464  2151  4809  2023  2003  1037 10433  2204  3185\n  1998  2323  2022  2464  2011  2035  1996  4268  1999  1996  2047  4245\n  2123  1005  1056  2131  2000  2156  2009  1998  1045  2228  2027  2323\n  2009  2323  2012  2560  2022  2404  2067  2006  1996  3149  2023  3185\n  2987  1005  1056 10107  1037 10036  1026  4895  2243  1028  2009 17210\n  1996  2613  2518  1045  1005  1049  2068  2085  2023  3185  2097  2022\n  2006  4966   102     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\n"
        }
      ],
      "source": [
        "print(X_test_input_ids[0])\n",
        "print(\"-\"*100)\n",
        "print(X_test_input_ids[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.  Create text Masks for BERT\n",
        "\n",
        "### Train Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_masks = BERT_get_masks_from_tokenized_data(data_tokens=X_train_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "261  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n----------------------------------------------------------------------------------------------------\n62  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_train_masks[0] == 1), \" | \", X_train_masks[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_train_masks[5] == 1), \" | \", X_train_masks[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_masks = BERT_get_masks_from_tokenized_data(data_tokens=X_test_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "97  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n----------------------------------------------------------------------------------------------------\n171  |  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_test_masks[0] == 1), \" | \", X_test_masks[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_test_masks[5] == 1), \" | \", X_test_masks[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create text Segments for BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_input_segments = BERT_get_segments_from_tokenized_data(data_tokens=X_train_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "512  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n----------------------------------------------------------------------------------------------------\n512  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_train_input_segments[0] == 0), \" | \", X_train_input_segments[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_train_input_segments[5] == 0), \" | \", X_train_input_segments[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_input_segments = BERT_get_segments_from_tokenized_data(data_tokens=X_test_tokens, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "512  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n----------------------------------------------------------------------------------------------------\n512  |  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
        }
      ],
      "source": [
        "print(sum(X_test_input_segments[0] == 0), \" | \", X_test_input_segments[0])\n",
        "print(\"-\"*100)\n",
        "print(sum(X_test_input_segments[5] == 0), \" | \", X_test_input_segments[5])"
      ]
    }
  ]
}