{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec(common_texts, size=300, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.2316521e-05,  4.7390778e-05,  4.3435142e-05, ...,\n",
       "       -4.7317448e-05,  3.7334288e-05, -4.8908816e-05], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['computer']  # numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words were not found in the vocabulary...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    w11 = model.wv['king']\n",
    "    w12 = model.wv['man']\n",
    "    w21 = model.wv['queen']\n",
    "    w22 = model.wv['woman']\n",
    "except:\n",
    "    print(\"Some words were not found in the vocabulary...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(wv.vocab))\n",
    "vec_king = wv['king']\n",
    "print(len(vec_king))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Paper Page 5 (Figure 1 New model architectures): \n",
    "* The CBOW architecture predicts the **current word based on the context**, and \n",
    "* The Skip-gram **predicts surrounding words given the current word**.\n",
    "\n",
    "My understanding:\n",
    "* So for CBOW, output should be a V sized softmax?\n",
    "* So for Skip-gram (lets say we pick C = 4 surrounding words +-2), output should be a of size 4 and each is a V sized Softmax (matches slide 10 from class PPT but contradicts slide 8.\n",
    "\n",
    "\n",
    "Slides from Class clarification:\n",
    "* Slide 8: Output (why does it have to be 1 for each vocab? Should it not just be a V sized softmax for the neighboring C words)\n",
    "\n",
    "\n",
    "Also \n",
    "* Slide 13, how does \"Dot Product/Similarity\" help? and what are we comparing this to?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question: Case matters below??\n",
    "\n",
    "four_grams = [\n",
    "    [('king', 'queen'), ('man', 'woman')],\n",
    "    [('king', 'man'), ('queen', 'woman')],\n",
    "    [('King', 'man'), ('Queen', 'woman')],\n",
    "    [('King', 'man'), ('queen', 'woman')],\n",
    "    [('man', 'woman'), ('boy', 'girl')],\n",
    "    [('Ottawa', 'Canada'), ('Nairobi', 'Kenya')],\n",
    "    [('big', 'bigger'), ('tall', 'taller')],\n",
    "    [('yen', 'japan'), ('ruble', 'russia')],\n",
    "    [('man', 'doctor'), ('woman', 'nurse')],  ## Bias in language  \n",
    "    [('Paris', 'France'), ('London', 'England')]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'king':'queen' as 'man': ?\n",
      "Ans: 'boy' with a similarity of 0.5393532514572144\n",
      "[('boy', 0.5393532514572144), ('guy', 0.47399765253067017), ('Alexios_Marakis', 0.4579210579395294), ('Man', 0.4575732350349426), ('teenager', 0.4346425235271454)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.33915412425994873\n",
      "Cosine Similarity using numpy: 0.33915409445762634\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'king':'man' as 'queen': ?\n",
      "Ans: 'queens' with a similarity of 0.595018744468689\n",
      "[('queens', 0.595018744468689), ('monarch', 0.5815044641494751), ('kings', 0.5612993240356445), ('royal', 0.5204525589942932), ('princess', 0.5191516876220703)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: -0.0818394273519516\n",
      "Cosine Similarity using numpy: -0.0818394273519516\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'King':'man' as 'Queen': ?\n",
      "Ans: 'Queen_Elizabeth' with a similarity of 0.48898202180862427\n",
      "[('Queen_Elizabeth', 0.48898202180862427), ('Mean_Lisa_Lampanelli', 0.4764140248298645), ('Mayfair_London_W1J', 0.4687288999557495), ('Rockabilly_Wanda_Jackson', 0.4648258090019226), ('Princess', 0.4587618112564087)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: -0.21346133947372437\n",
      "Cosine Similarity using numpy: -0.21346135437488556\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'King':'man' as 'queen': ?\n",
      "Ans: 'Queen' with a similarity of 0.5981242656707764\n",
      "[('Queen', 0.5981242656707764), ('king', 0.5400344133377075), ('queens', 0.5190572738647461), ('monarch', 0.4982725977897644), ('royal', 0.46667739748954773)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: -0.14685717225074768\n",
      "Cosine Similarity using numpy: -0.14685718715190887\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'man':'woman' as 'boy': ?\n",
      "Ans: 'teenager' with a similarity of 0.6156192421913147\n",
      "[('teenager', 0.6156192421913147), ('kid', 0.6022343635559082), ('lad', 0.6012977361679077), ('schoolboy', 0.5923386812210083), ('youngster', 0.5844612717628479)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.5397276878356934\n",
      "Cosine Similarity using numpy: 0.5397276878356934\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'Ottawa':'Canada' as 'Nairobi': ?\n",
      "Ans: 'Kampala' with a similarity of 0.662186861038208\n",
      "[('Kampala', 0.662186861038208), ('Dar_es_Salaam', 0.627353310585022), ('Nairobi_Kenya', 0.6136585474014282), ('Addis_Ababa', 0.6082125902175903), ('Lusaka', 0.6080822348594666)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.4288235902786255\n",
      "Cosine Similarity using numpy: 0.42882347106933594\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'big':'bigger' as 'tall': ?\n",
      "Ans: 'tall_skinny' with a similarity of 0.5383595824241638\n",
      "[('tall_skinny', 0.5383595824241638), ('feet_tall', 0.5371847748756409), ('lanky', 0.5139034986495972), ('stocky', 0.5098012089729309), ('towering', 0.5037447214126587)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.34425151348114014\n",
      "Cosine Similarity using numpy: 0.3442515432834625\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'yen':'japan' as 'ruble': ?\n",
      "Ans: 'hryvnia' with a similarity of 0.6195263862609863\n",
      "[('hryvnia', 0.6195263862609863), ('forint', 0.5946279764175415), ('euro', 0.5857446193695068), ('zloty', 0.5648157000541687), ('rupiah', 0.56055748462677)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.02923150360584259\n",
      "Cosine Similarity using numpy: 0.02923150174319744\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'man':'doctor' as 'woman': ?\n",
      "Ans: 'teenage_girl' with a similarity of 0.5961461067199707\n",
      "[('teenage_girl', 0.5961461067199707), ('girl', 0.5741775631904602), ('teenager', 0.5395423769950867), ('boy', 0.5295058488845825), ('suspected_purse_snatcher', 0.4840123653411865)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: -0.020860131829977036\n",
      "Cosine Similarity using numpy: -0.020860126242041588\n",
      "--------------------------------------------------\n",
      "1.0 Using inbuilt functions to find most similar word ...\n",
      "'Paris':'France' as 'London': ?\n",
      "Ans: 'Londons' with a similarity of 0.5508409738540649\n",
      "[('Londons', 0.5508409738540649), ('Canary_Wharf', 0.5483355522155762), ('Islamabad_Slyvia_Hui', 0.5476956963539124), ('Canary_Warf', 0.544357180595398), ('EURASIAN_NATURAL_RESOURCES_CORP.', 0.5380204319953918)]\n",
      "\n",
      "2.0 Using manual vecotor addition/subtraction ...\n",
      "Cosine Similarity using scipy: 0.14545515179634094\n",
      "Cosine Similarity using numpy: 0.14545515179634094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "for four_gram in four_grams:\n",
    "    lhs = wv[four_gram[0][0]] - wv[four_gram[0][1]] + wv[four_gram[1][0]]\n",
    "    rhs = wv[four_gram[1][1]]  \n",
    "    print(\"-\"*50)\n",
    "    print(\"1.0 Using inbuilt functions to find most similar word ...\")\n",
    "    top_similar = wv.most_similar(positive=[four_gram[0][0], four_gram[1][0]], negative=[four_gram[0][1]], topn=5)\n",
    "    print(f\"'{four_gram[0][0]}':'{four_gram[0][1]}' as '{four_gram[1][0]}': ?\")\n",
    "    print(f\"Ans: '{top_similar[0][0]}' with a similarity of {top_similar[0][1]}\")\n",
    "    print(top_similar)\n",
    "    \n",
    "    print(\"\\n2.0 Using manual vecotor addition/subtraction ...\")\n",
    "   \n",
    "    # gensim_similarity = 1 - wv.distance(lhs, rhs)\n",
    "    # print(f\"Similarity using Gensim: {gensim_similarity}\")\n",
    "    cosine_similarity1 = 1 - spatial.distance.cosine(lhs, rhs)\n",
    "    print(f\"Cosine Similarity using scipy: {cosine_similarity1}\")\n",
    "    cosine_similarity2 = np.dot(lhs, rhs)/(norm(lhs)*norm(rhs))\n",
    "    print(f\"Cosine Similarity using numpy: {cosine_similarity2}\")    \n",
    "    \n",
    "    # print(softcossim(lhs,rhs, wv.similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mexponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnonzero_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float32'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Construct a term similarity matrix for computing Soft Cosine Measure.\n",
       "\n",
       "This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
       "Soft Cosine Measure between documents.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
       "    A dictionary that specifies the considered terms.\n",
       "tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
       "    A model that specifies the relative importance of the terms in the dictionary. The\n",
       "    columns of the term similarity matrix will be build in a decreasing order of importance\n",
       "    of terms, or in the order of term identifiers if None.\n",
       "threshold : float, optional\n",
       "    Only embeddings more similar than `threshold` are considered when retrieving word\n",
       "    embeddings closest to a given word embedding.\n",
       "exponent : float, optional\n",
       "    Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
       "nonzero_limit : int, optional\n",
       "    The maximum number of non-zero elements outside the diagonal in a single column of the\n",
       "    sparse term similarity matrix.\n",
       "dtype : numpy.dtype, optional\n",
       "    Data-type of the sparse term similarity matrix.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`scipy.sparse.csc_matrix`\n",
       "    Term similarity matrix.\n",
       "\n",
       "See Also\n",
       "--------\n",
       ":func:`gensim.matutils.softcossim`\n",
       "    The Soft Cosine Measure.\n",
       ":class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
       "    A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
       "`Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
       "between Questions for Community Question Answering\", 2017\n",
       "<http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?wv.similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelative_cosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Compute the relative cosine similarity between two words given top-n similar words,\n",
       "by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
       "for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
       "\n",
       "To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
       "For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
       "any arbitrary word pairs.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "wa: str\n",
       "    Word for which we have to look top-n similar word.\n",
       "wb: str\n",
       "    Word for which we evaluating relative cosine similarity with wa.\n",
       "topn: int, optional\n",
       "    Number of top-n similar words to look with respect to wa.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "numpy.float64\n",
       "    Relative cosine similarity between wa and wb.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\nikhil\\.conda\\envs\\dl_nlp\\lib\\site-packages\\gensim\\models\\keyedvectors.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?wv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)",
   "language": "python",
   "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
