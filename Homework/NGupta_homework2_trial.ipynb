{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
    }
   ],
   "source": [
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = get_tmpfile(\"word2vec.model\")\n",
    "# model = Word2Vec(common_texts, size=300, window=5, min_count=1, workers=4)\n",
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0, 2)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(300,)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "vector = model.wv['computer']  # numpy vector of a word\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Some words were not found in the vocabulary...\n"
    }
   ],
   "source": [
    "try:\n",
    "    w11 = model.wv['king']\n",
    "    w12 = model.wv['man']\n",
    "    w21 = model.wv['queen']\n",
    "    w22 = model.wv['woman']\n",
    "except:\n",
    "    print(\"Some words were not found in the vocabulary...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "### https://radimrehurek.com/gensim/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Too slow to load each time. Is there a way to store this?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3000000\n300\n"
    }
   ],
   "source": [
    "print(len(wv.vocab))\n",
    "vec_king = wv['king']\n",
    "print(len(vec_king))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'red'>My Questions</font>\n",
    "\n",
    "From Paper Page 5 (Figure 1 New model architectures): \n",
    "* The CBOW architecture predicts the **current word based on the context**, and \n",
    "* The Skip-gram **predicts surrounding words given the current word**.\n",
    "\n",
    "My understanding:\n",
    "* So for CBOW, output should be a V sized softmax?\n",
    "* So for Skip-gram (lets say we pick C = 4 surrounding words +-2), output should be a of size 4 and each is a V sized Softmax (matches slide 10 from class PPT but contradicts slide 8.\n",
    "\n",
    "\n",
    "Slides from Class clarification:\n",
    "* Slide 8: Output (why does it have to be 1 for each vocab? Should it not just be a V sized softmax for the neighboring C words)\n",
    "\n",
    "\n",
    "Also \n",
    "* Slide 13, how does \"Dot Product/Similarity\" help? and what are we comparing this to?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='red'> Homework related questions: </font>\n",
    "\n",
    "1. Question 1: How do you want us to compare the vectors (they are 300x1)? Are you looking for a similarity measure like cosine similarity? --> use the one provided by the library. Gensim --> ?wv.relative_cosine_similarity\n",
    "2. Question 4 and 5, are we supposed to only calculate the distance for Spacy and not for Gensim? Is Gensim same as manual and scipy (does not look like it below).\n",
    "\n",
    "Question 3: Are the vecotors the same?\n",
    "\n",
    "### <font color ='red'> More General Questions: </font>\n",
    "1. Case matters below?\n",
    "2. Debiasing?\n",
    "4. Concept of soft cosine similarity and relative cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_grams = [\n",
    "    [('king', 'queen'), ('man', 'woman')],\n",
    "    [('king', 'man'), ('queen', 'woman')],\n",
    "    [('King', 'man'), ('Queen', 'woman')],\n",
    "    [('King', 'man'), ('queen', 'woman')],\n",
    "    [('man', 'woman'), ('boy', 'girl')],\n",
    "    [('Ottawa', 'Canada'), ('Nairobi', 'Kenya')],\n",
    "    [('big', 'bigger'), ('tall', 'taller')],\n",
    "    [('yen', 'japan'), ('ruble', 'russia')],\n",
    "    [('man', 'doctor'), ('woman', 'nurse')],  ## Bias in language  \n",
    "    [('France', 'Paris'), ('England', 'London')]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6500488"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "calc = wv['Paris'] - wv['France'] + wv['England']\n",
    "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "cosine(calc,wv['London'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: king woman | Negative: queen\n'queen:king' as 'woman': ?\nAns: 'man' with a similarity of 0.72110915184021\n[('man', 0.72110915184021), ('boy', 0.5595242977142334), ('teenage_girl', 0.513959527015686), ('girl', 0.49721550941467285), ('teenager', 0.4869248569011688)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.6913302540779114\nCosine Similarity using numpy: 0.6913303732872009\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: king woman | Negative: man\n'man:king' as 'woman': ?\nAns: 'queen' with a similarity of 0.7118192911148071\n[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7300516963005066\nCosine Similarity using numpy: 0.7300517559051514\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: King woman | Negative: man\n'man:King' as 'woman': ?\nAns: 'Queen' with a similarity of 0.5515626072883606\n[('Queen', 0.5515626072883606), ('Oprah_BFF_Gayle', 0.47597548365592957), ('Geoffrey_Rush_Exit', 0.46460166573524475), ('Princess', 0.4533674716949463), ('Yvonne_Stickney', 0.45070409774780273)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.55495285987854\nCosine Similarity using numpy: 0.55495285987854\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: King woman | Negative: man\n'man:King' as 'woman': ?\nAns: 'Queen' with a similarity of 0.5515626072883606\n[('Queen', 0.5515626072883606), ('Oprah_BFF_Gayle', 0.47597548365592957), ('Geoffrey_Rush_Exit', 0.46460166573524475), ('Princess', 0.4533674716949463), ('Yvonne_Stickney', 0.45070409774780273)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.4479384422302246\nCosine Similarity using numpy: 0.4479384422302246\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: man girl | Negative: woman\n'woman:man' as 'girl': ?\nAns: 'boy' with a similarity of 0.8748372793197632\n[('boy', 0.8748372793197632), ('teenager', 0.6932152509689331), ('teenage_girl', 0.6277071237564087), ('lad', 0.624101996421814), ('kid', 0.6167631149291992)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.8843864798545837\nCosine Similarity using numpy: 0.884386420249939\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: Ottawa Kenya | Negative: Canada\n'Canada:Ottawa' as 'Kenya': ?\nAns: 'Nairobi' with a similarity of 0.6801793575286865\n[('Nairobi', 0.6801793575286865), ('Kenyan', 0.6379503011703491), ('Kisumu', 0.6098698377609253), ('Kampala', 0.5967018604278564), ('Naivasha', 0.5881980657577515)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7072624564170837\nCosine Similarity using numpy: 0.7072624564170837\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: big taller | Negative: bigger\n'bigger:big' as 'taller': ?\nAns: 'tall' with a similarity of 0.7226848602294922\n[('tall', 0.7226848602294922), ('feet_tall', 0.5686451196670532), ('tall_skinny', 0.5638656616210938), ('inches_taller', 0.559072732925415), ('tallest', 0.5533725023269653)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7648589611053467\nCosine Similarity using numpy: 0.7648590207099915\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: yen russia | Negative: japan\n'japan:yen' as 'russia': ?\nAns: 'euro' with a similarity of 0.5616427659988403\n[('euro', 0.5616427659988403), ('ruble', 0.5292985439300537), ('rubles', 0.49458637833595276), ('JPY', 0.49233901500701904), ('franc', 0.49085041880607605)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.5363868474960327\nCosine Similarity using numpy: 0.5363868474960327\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: man nurse | Negative: doctor\n'doctor:man' as 'nurse': ?\nAns: 'woman' with a similarity of 0.6514645218849182\n[('woman', 0.6514645218849182), ('teenage_girl', 0.5248744487762451), ('girl', 0.522849440574646), ('suspected_purse_snatcher', 0.5100316405296326), ('boy', 0.5027773380279541)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.5958573222160339\nCosine Similarity using numpy: 0.5958573222160339\n--------------------------------------------------\n1.0 Using inbuilt functions to find most similar word ...\nPositive: France London | Negative: Paris\n'Paris:France' as 'London': ?\nAns: 'Britain' with a similarity of 0.7368935346603394\n[('Britain', 0.7368935346603394), ('UK', 0.6637030243873596), ('England', 0.6119861006736755), ('United_Kingdom', 0.6067783832550049), ('Great_Britain', 0.5870823860168457)]\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.6076576709747314\nCosine Similarity using numpy: 0.6076576709747314\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "for four_gram in four_grams:\n",
    "    lhs = wv[four_gram[0][0]] - wv[four_gram[0][1]] + wv[four_gram[1][1]]\n",
    "    rhs = wv[four_gram[1][0]]  \n",
    "    print(\"-\"*50)\n",
    "    print(\"1.0 Using inbuilt functions to find most similar word ...\")\n",
    "    print(f\"Positive: {four_gram[0][0]} {four_gram[1][1]} | Negative: {four_gram[0][1]}\")\n",
    "    top_similar = wv.most_similar(positive=[four_gram[0][0], four_gram[1][1]], negative=[four_gram[0][1]], topn=5)\n",
    "    print(f\"'{four_gram[0][1]}:{four_gram[0][0]}' as '{four_gram[1][1]}': ?\")\n",
    "    print(f\"Ans: '{top_similar[0][0]}' with a similarity of {top_similar[0][1]}\")\n",
    "    print(top_similar)\n",
    "    \n",
    "    print(\"\\n2.0 Using manual vecotor addition/subtraction ...\")\n",
    "   \n",
    "    # # Gensim diatance function needs \"words\" as input not \"vectors\"\n",
    "    # gensim_similarity = 1 - wv.distance(lhs, rhs)\n",
    "    # print(f\"Similarity using Gensim: {gensim_similarity}\")\n",
    "    cosine_similarity1 = 1 - spatial.distance.cosine(lhs, rhs)  ## Scipy\n",
    "    print(f\"Cosine Similarity using scipy: {cosine_similarity1}\")\n",
    "    cosine_similarity2 = np.dot(lhs, rhs)/(norm(lhs)*norm(rhs))\n",
    "    print(f\"Cosine Similarity using numpy: {cosine_similarity2}\")    \n",
    "    \n",
    "    # print(softcossim(lhs,rhs, wv.similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('London', 0.6441212892532349),\n ('stock_symbol_BNK', 0.5432794094085693),\n ('ticker_symbol_BNK', 0.5106071829795837),\n ('LSO_St_Lukes', 0.474970281124115),\n ('Leeds', 0.465692400932312),\n ('Englands', 0.4634588956832886),\n ('Islamabad_Slyvia_Hui', 0.46285343170166016),\n ('Manchester', 0.4587923288345337),\n ('Covent_Garden', 0.4549728333950043),\n ('Kensington', 0.4531983435153961)]"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "wv.most_similar(['Paris','England'], ['France'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?wv.similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?wv.relative_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?softcossim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "### https://spacy.io/usage/vectors-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dog True 7.0336733 False (300,)\ncat True 6.6808186 False (300,)\nbanana True 6.700014 False (300,)\nafskfsd False 0.0 True (300,)\n"
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "# Text: The original token text.\n",
    "# has vector: Does the token have a vector representation?\n",
    "# Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared)\n",
    "# OOV: Out-of-vocabulary\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov, token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dog dog 1.0\ndog cat 0.80168545\ndog banana 0.24327643\ncat dog 0.80168545\ncat cat 1.0\ncat banana 0.28154364\nbanana dog 0.24327643\nbanana cat 0.28154364\nbanana banana 1.0\n"
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mDocstring:\u001b[0m\nMake a semantic similarity estimate. The default estimate is cosine\nsimilarity using an average of word vectors.\n\nother (object): The object to compare with. By default, accepts `Doc`,\n    `Span`, `Token` and `Lexeme` objects.\nRETURNS (float): A scalar similarity score. Higher is more similar.\n\nDOCS: https://spacy.io/api/doc#similarity\n\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
    }
   ],
   "source": [
    "?token1.similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(300,)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "token1.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9088526108382569"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "doc1 = nlp(\"I hate cats\")\n",
    "doc2 = nlp(\"I like dogs\")\n",
    "\n",
    "doc1.similarity(doc2)\n",
    "\n",
    "# # Analyzing survey data\n",
    "# doc1 = nlp(\"I returned to India because of family reasons. I had older parents who needed help.\")\n",
    "# doc2 = nlp(\"I decided to stay back in the US because of more opportunities.\")\n",
    "# doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_grams = [\n",
    "    ['king', 'queen', 'man', 'woman'],\n",
    "    ['king', 'man', 'queen', 'woman'],\n",
    "    ['King', 'man', 'Queen', 'woman'],\n",
    "    ['King', 'man', 'queen', 'woman'],\n",
    "    ['man', 'woman', 'boy', 'girl'],\n",
    "    ['Ottawa', 'Canada', 'Nairobi', 'Kenya'],\n",
    "    ['big', 'bigger', 'tall', 'taller'],\n",
    "    ['yen', 'japan', 'ruble', 'russia'],\n",
    "    ['man', 'doctor', 'woman', 'nurse'],  ## Bias in language  \n",
    "    ['Paris', 'France', 'London', 'England']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['king', 'queen', 'man', 'woman']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7820168733596802\nCosine Similarity using numpy: 0.7820168137550354\n['king', 'man', 'queen', 'woman']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7880843877792358\nCosine Similarity using numpy: 0.7880844473838806\n['King', 'man', 'Queen', 'woman']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7880843877792358\nCosine Similarity using numpy: 0.7880844473838806\n['King', 'man', 'queen', 'woman']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7880843877792358\nCosine Similarity using numpy: 0.7880844473838806\n['man', 'woman', 'boy', 'girl']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.832800567150116\nCosine Similarity using numpy: 0.8328006863594055\n['Ottawa', 'Canada', 'Nairobi', 'Kenya']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.6792001724243164\nCosine Similarity using numpy: 0.6792001724243164\n['big', 'bigger', 'tall', 'taller']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7592806220054626\nCosine Similarity using numpy: 0.7592805624008179\n['yen', 'japan', 'ruble', 'russia']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.6422593593597412\nCosine Similarity using numpy: 0.6422594785690308\n['man', 'doctor', 'woman', 'nurse']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.6244680881500244\nCosine Similarity using numpy: 0.6244680285453796\n['Paris', 'France', 'London', 'England']\n\n2.0 Using manual vecotor addition/subtraction ...\nCosine Similarity using scipy: 0.7661835551261902\nCosine Similarity using numpy: 0.7661834955215454\n"
    }
   ],
   "source": [
    "for four_gram in four_grams:\n",
    "    print(four_gram)\n",
    "    token1 = nlp(four_gram[0])\n",
    "    token2 = nlp(four_gram[1])\n",
    "    token3 = nlp(four_gram[2])\n",
    "    token4 = nlp(four_gram[3])\n",
    "    lhs = token1.vector - token2.vector + token4.vector\n",
    "    rhs = token3.vector\n",
    "    \n",
    "\n",
    "    print(\"\\n2.0 Using manual vecotor addition/subtraction ...\")\n",
    "    cosine_similarity1 = 1 - spatial.distance.cosine(lhs, rhs)\n",
    "    print(f\"Cosine Similarity using scipy: {cosine_similarity1}\")\n",
    "    cosine_similarity2 = np.dot(lhs, rhs)/(norm(lhs)*norm(rhs))\n",
    "    print(f\"Cosine Similarity using numpy: {cosine_similarity2}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('dl_nlp': conda)",
   "language": "python",
   "name": "python361064bitdlnlpconda289d61a8916e4086b0ec12c871e554fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}